cd(dirname(@__FILE__))
cd("..")

using Pkg
Pkg.activate(".")  # activate this project's environment
Pkg.instantiate()  # make sure all packages installed

using Random, IrrationalConstants, Format, Distributions, Interpolations, Base.Iterators, FastGaussQuadrature, Optim, LogExpFunctions, CSV, DataFrames, DataFramesMeta, ForwardDiff, LinearAlgebra, Roots, QuadGK, Statistics, 
       InverseFunctions, StatsAPI, StatsBase, StatsModels, RegressionTables, Unicode, CairoMakie, Makie, ExcelFiles, XLSX, RData, SpecialFunctions, ThreadsX, HCubature

const ğ’© = Normal()
const zÌ„ = quantile(ğ’©, .975)  # 1.96

@inline diffcdf(N,b,a) = cdf(N,b) - cdf(N,a)
@inline sqrt0(x::T) where {T} = x<0 ? zero(T) : sqrt(x)


#
# generalized t distribution: adds Î¼ and Ïƒ parameters
#
struct GenT{T<:Real} <: ContinuousUnivariateDistribution
	Î¼::T; Ïƒ::T; Î½::T

	lnÏƒ::T
	tdist::TDist{T}  # underlying Student's t distribution

	GenT(Î¼::T, Ïƒ::T, Î½::T) where {T<:Real} = new{T}(Î¼, Ïƒ, Î½, log(Ïƒ), TDist{T}(Î½))
end
Distributions.pdf(     d::GenT, x::Real) = pdf(     d.tdist, (x - d.Î¼) / d.Ïƒ) / d.Ïƒ
Distributions.logpdf(  d::GenT, x::Real) = logpdf(  d.tdist, (x - d.Î¼) / d.Ïƒ) - d.lnÏƒ
Distributions.cdf(     d::GenT, x::Real) = cdf(     d.tdist, (x - d.Î¼) / d.Ïƒ)
Distributions.logcdf(  d::GenT, x::Real) = logcdf(  d.tdist, (x - d.Î¼) / d.Ïƒ)
Distributions.quantile(d::GenT, p::Real) = quantile(d.tdist, p) * d.Ïƒ + d.Î¼


# to parameterize an n-vector of probabilities summing to 1 with an unbounded (n-1)-vector, apply logistic transform to latter, then map to squared spherical coordinates
# https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates, https://math.stackexchange.com/questions/2861449/parameterizations-of-the-unit-simplex-in-mathbbr3
function Râ¿toSimplex(q::AbstractVector{T}) where {T}
	if iszero(length(q))
		T[1]
	elseif isone(length(q))
		t = cospi(logistic(q[]))^2 |> (x -> isnan(x) ? zero(T) : x)
		T[t, 1-t]
	else
		p = Vector{T}(undef, length(q)+1)
		Î sinÂ² = one(T)
		@inbounds for i âˆˆ eachindex(q)
			sinÂ², cosÂ² = (q[i] |> logistic |> sincospi).^2
			p[i] = Î sinÂ² * cosÂ²
			Î sinÂ² *= sinÂ²
		end
		p[end] = Î sinÂ²
		replace!(p, NaN=>0)
	end
end
function SimplextoRâ¿(p::AbstractVector{T}) where {T}
	q = Vector{T}(undef, length(p)-1)
	sum = p[end]
	@inbounds for i âˆˆ reverse(eachindex(q))
		sum += p[i]
		q[i] = acos(âˆš(p[i] / sum)) / Ï€
	end
	q .= logit.(q)
end
InverseFunctions.inverse(::typeof(SimplextoRâ¿)) = Râ¿toSimplex

# transform to constrain parameters
get0(::Vector{T}) where {T} = T[]
put0(::Vector{T}) where {T} = T[0]  # constant 0
InverseFunctions.inverse(::typeof(get0)) = put0
get1(::Vector{T}) where {T} = T[]
put1(::Vector{T}) where {T} = T[1]  # constant 1
InverseFunctions.inverse(::typeof(get1)) = put1
get1000(::Vector{T}) where {T} = T[]
put1000(::Vector{T}) where {T} = T[1,0,0,0]  # constant 1,0,0,0
InverseFunctions.inverse(::typeof(get1000)) = put1000

# # transform to constrain pDFHR to have pR=0
# get_pR0(v::Vector{T}) where {T} = v[1:3]
# put_pR0(v::Vector{T}) where {T} = T[v; 0]
# InverseFunctions.inverse(::typeof(get_pR0)) = put_pR0
# get_pF0(v::Vector{T}) where {T} = [v[1]; v[3:4]]
# put_pF0(v::Vector{T}) where {T} = [v[1]; 0; v[2:3]]
# InverseFunctions.inverse(::typeof(get_pF0)) = put_pF0
# get_pHR0(v::Vector{T}) where {T} = v[1:2]
# put_pHR0(v::Vector{T}) where {T} = T[v; 0; 0]
# InverseFunctions.inverse(::typeof(get_pHR0)) = put_pHR0
# get_pDR0(v::Vector{T}) where {T} = v[2:3]
# put_pDR0(v::Vector{T}) where {T} = T[0; v; 0]
# InverseFunctions.inverse(::typeof(get_pDR0)) = put_pDR0

# functions to map x <-> fill(x,k) for k=1,2,3,4
shared1(x) = [x[1]]
shared2(x) = [x[1]]
shared3(x) = [x[1]]
shared4(x) = [x[1]]
const shared = shared1, shared2, shared3, shared4
fill1(x) = fill(x[],1)
fill2(x) = fill(x[],2)
fill3(x) = fill(x[],3)
fill4(x) = fill(x[],4)
InverseFunctions.inverse(::typeof(shared1)) = fill1
InverseFunctions.inverse(::typeof(shared2)) = fill2
InverseFunctions.inverse(::typeof(shared3)) = fill3
InverseFunctions.inverse(::typeof(shared4)) = fill4

log1m(x) = log(x - 1)
expp1(x) = exp(x) + 1
InverseFunctions.inverse(::typeof(log1m)) = expp1

bcast = Broadcast.BroadcastFunction  # short-hand for forming the broadcasting version of a function, which works with InverseFunctions


# compute f(z|Ï‰) & F(file drawer|Ï‰). Return in provided 2-vector y
function _fZcondÎ©!(y, z, Ï‰; modelabsz=false, NLegendre=50, pDFHR::Vector{T}, Ïƒ::Vector{T}, m::Vector{T}) where {T}
	pD, _, pH, pR = pDFHR
  lnpH = log(pH)

	Zâ‚€, W = gausslegendre(NLegendre)  # nodes and weights for Gauss-Legendre quadrature over [-1,1]
	Zâ‚€ .*= zÌ„  # change of variables to quadrature over [-zÌ„, zÌ„]
  lnWLegendre = log.(W) .+ log(zÌ„)

	zdivÏƒ, zÌ„divÏƒ = z/Ïƒ[], zÌ„/Ïƒ[]

  file_drawer = âˆ« = 0.
	b = zdivÏƒ; absb = abs(b)
	@inbounds for k âˆˆ 1:NLegendre  # p-hacking; integrate out zâ‚€ over [-zÌ„, zÌ„]
		a = Zâ‚€[k] / Ïƒ[]
    B = lnWLegendre[k] + logpdf(ğ’©, Zâ‚€[k]-Ï‰) - log1mexp(lnpH + logdiffcdf(ğ’©, a+zÌ„divÏƒ, a-zÌ„divÏƒ) * m[])
    file_drawer += exp(B) 

		if a+absb â‰‰ a-absb
      F = logpdf(ğ’©, b-a) + logdiffcdf(ğ’©, a+absb, a-absb) * (m[]-1)
			modelabsz && (F += log1pexp(-2b * a))  # log [Ï•(a-b) + Ï•(a+b)] = log[Ï•(a-b)] + log[1+exp(-2ab)]
			âˆ« += exp(B + F)
		end
	end
	âˆ« *= m[] / Ïƒ[] * pH  # density contribution from p-hacking

	f_z = pdf(ğ’©, z-Ï‰)
	modelabsz && (f_z += pdf(ğ’©, z+Ï‰))

	âˆ« += f_z  # contribution from publishing original stat without p-hacking
	if -zÌ„ â‰¤ z â‰¤ zÌ„
		âˆ« *= pD  # in insignificant range, same formulas, but times pD
    âˆ« += pR * f_z / (1 - exp(lnpH + logdiffcdf(ğ’©, zdivÏƒ+zÌ„divÏƒ, zdivÏƒ-zÌ„divÏƒ) * m[]))  # contribution from reverting to original stat after p-hacking
	end
	y .= âˆ«, file_drawer
end

_fZcondÎ©(z, Ï‰; kwargs...) = _fZcondÎ©!(Vector{Float64}(undef,2), z, Ï‰; kwargs...)

 # f(z|Ï‰). If truncate=true (the default), returns the density conditional on publication
fZcondÎ©(z, Ï‰; modelabsz=false, NLegendre=50, pDFHR, Ïƒ, m, truncate=true) = _fZcondÎ©(z, Ï‰; modelabsz, NLegendre, pDFHR, Ïƒ, m) |> (y -> truncate ? y[1]/(1 - pDFHR[2]*y[2]) : y[1])
 
# the most time-consuming plotting is of the confidence intervals: for various values of Ï‰, 
# the cdf F(z|Ï‰) is numerically calculated, many times--iteratively seeking where it hits, e.g., .025 and .975
# to save time, pre-compute all components of f(z|Ï‰) that do not depend on z, notably logdiffcdf(ğ’©(0,Ïƒ), Zâ‚€[k]+zÌ„, Zâ‚€[k]-zÌ„)
function FZcondÎ©(z, Ï‰; modelabsz::Bool=false, NLegendre=50, pDFHR, Ïƒ, m, rtol=.00001, order=13)
	pD, pF, pH, pR = pDFHR
  lnpH = log(pH)

	Zâ‚€, W = gausslegendre(NLegendre)  # nodes and weights for Gauss-Legendre quadrature over [-1,1]
	Zâ‚€ .*= zÌ„  # change of variables to quadrature over [-zÌ„, zÌ„]
	W  .*= zÌ„
	
	zÌ„divÏƒ, Zâ‚€divÏƒ = zÌ„/Ïƒ[], Zâ‚€/Ïƒ[]

	A = 0.
	B = Vector{Float64}(undef, NLegendre)
	@inbounds for k âˆˆ 1:NLegendre
		a = Zâ‚€[k] / Ïƒ[]
		B[k] = log(W[k]) + logpdf(ğ’©, Zâ‚€[k] - Ï‰) - log1mexp(lnpH + logdiffcdf(ğ’©, a+zÌ„divÏƒ, a-zÌ„divÏƒ) * m[])
		A += exp(B[k])
	end

	function myfZcondÎ©(z)
		zdivÏƒ = z / Ïƒ[]
		b = abs(zdivÏƒ)

		âˆ« = 0.
		@inbounds for k âˆˆ 1:NLegendre
			a = Zâ‚€divÏƒ[k]
			if a+b â‰‰ a-b
				Fâ‚– = -.5(zdivÏƒ - a)^2 + (m[]-1) * logdiffcdf(ğ’©, a+b, a-b)  # p_H Ï•(z;z_0,Ïƒ^2 )
				modelabsz && (Fâ‚– += log1pexp(-2a * b))  # log [Ï•(a-b) + Ï•(a+b)] = log[Ï•(a-b)] + log[1+exp(-2ab)]
				âˆ« += exp(B[k] + Fâ‚–)
			end
		end
		âˆ« *= pH / Ïƒ[] * m[]

		f_z = exp(-.5(z-Ï‰)^2)
		modelabsz && (f_z += exp(-.5(z+Ï‰)^2))

		âˆ« += f_z
		if -zÌ„ â‰¤ z â‰¤ zÌ„
			âˆ« *= pD
      âˆ« += pR * f_z / (1 - pH * diffcdf(ğ’©, zdivÏƒ+zÌ„divÏƒ, zdivÏƒ-zÌ„divÏƒ) ^ m[])
		end
		âˆ«
	end

	endpoints = modelabsz ? [0, zÌ„] : [-Inf, -zÌ„, zÌ„]  # since f(z|Ï‰) jumps at Â±zÌ„, do quadrature separately in each range
	endpoints = [endpoints[findall(<(z), endpoints)]; z]
	quadgk(myfZcondÎ©, endpoints...; rtol, order)[1] * invsqrt2Ï€ / (1 - pF * A)
end

quantFcondÎ©(q, Ï‰; kwargs...) = find_zero(z -> q - FZcondÎ©(z, Ï‰; kwargs...), (-20,20), Roots.ITP())  # ITP algorithm works well

# likelihood for a collection (vector, step range) of z's for plotting
# If truncate=true (default), returns the truncated density, i.e., conditional on publication
function fZ(z; modelabsz=false, NHermite=50, NLegendre=50, p, Î¼, Ï„, Î½, pDFHR, Ïƒ, m, truncate=true)
  M = HnFmodel(z; d=length(Ï„), NHermite, NLegendre, modelabsz)
  âˆ«, G = _HnFll(M; p,Î¼,Ï„,Î½,pDFHR,Ïƒ,m)
	âˆ« .= exp.(âˆ«)
  truncate && (âˆ« ./= 1 - pDFHR[2]*G)
  âˆ«
end


# f(z), f(Ï‰), f(Ï‰|z), E[Ï‰|z]
# inconsistency: z should be a scalar for fÎ©condZ but a vector or other iterable for EÎ©condZ
fÎ©(Ï‰; p, Î¼, Ï„, Î½) = p'pdf.(GenT.(Î¼,Ï„,Î½), Ï‰)
fÎ©condZ(Ï‰, z; p, Î¼, Ï„, Î½, NHermite=50, NLegendre=50, kwargs...) = fZcondÎ©(z, Ï‰; NLegendre, kwargs..., truncate=false) * fÎ©(Ï‰; p, Î¼, Ï„, Î½) / fZ([z]; p, Î¼, Ï„, Î½, kwargs..., NLegendre, NHermite, truncate=false)[]
EÎ©condZ(z; rtol=.00001, maxevals=1e4, p, Î¼, Ï„, Î½, NHermite=50, NLegendre=50, kwargs...) = [quadgk(Ï‰ -> Ï‰ * fZcondÎ©(záµ¢, Ï‰; kwargs..., NLegendre, truncate=false) * fÎ©(Ï‰; p, Î¼, Ï„, Î½), -20, 20; rtol, maxevals)[1] for záµ¢âˆˆz] ./ 
                                                                      fZ(z; p, Î¼, Ï„, Î½, kwargs..., NLegendre, NHermite, truncate=false)

# CIs
Cquant(Î±, z; kwargs...) = find_zero(Ï‰ -> Î± - FZcondÎ©(z, Ï‰; kwargs...), (-20,20), Roots.ITP())  # Andrews & Kasy (2019), eq. 2
CI(    Î±, z; kwargs...) = Cquant(Î±/2, z; kwargs...), Cquant(1-Î±/2, z; kwargs...)


# object to hold pre-computed stuff for hack'n'file log likelihood computation
struct HnFmodel
	modelabsz::Bool  # modeling |z|?
	d::Vector{Int}  # number of mixture components; scalar stored as mutable vector
	z::Vector{Float64}  # all data
	wt::Vector{Float64}  # observation weights
	N::Int  # number of z's in data, # of insignificant
	k::Int  # number of z knots for interpolation
	interpolate::Bool	# interpolation resolution (points per unit interval); 0 means no interpolation
	kts::Vector{Float64}  # interpolation knots/observations in z space
	insig::BitVector  # which knots are in insignificant region
	splinetype::Interpolations.InterpolationType  # type of interpolation
	zint::Vector{Float64}  # z values mapped to cardinal knot numbering space since interpolate() is faster with cardinally spaced knots
	NHermite::Int  # number of quadrature points for integration over zâ‚€ to compute f(zâ‚€)
	Î©::Vector{Float64}; WHermite::Vector{Float64}; lnWpÎ©Â²::Vector{Float64}  # quadrature nodes & weights
	NLegendre::Int  # number of quadrature points
	Zâ‚€::Vector{Float64}; WLegendre::Vector{Float64}; lnWLegendre::Vector{Float64}  # quadrature nodes & weights
  penalty::Function
	Bdict::Dict{DataType, Vector}  # collections of pre-allocated arrays for use in likelihood computation, separate for Float64, ForwardDiff.Dual, etc.
	Edict::Dict{DataType, Vector}
	Fdict::Dict{DataType, Matrix}
	tot_hacking_dict::Dict{DataType, Vector}
	âˆ«dict::Dict{DataType, Matrix}

	function HnFmodel(z, wt=Float64[]; d::Int, modelabsz=false, interpres=0, NHermite=50, NLegendre=50, splinetype::Interpolations.InterpolationType=BSpline(Linear()), 
                    penalty::Function=(; kwargs...)->0.)
		if iszero(interpres)
			kts = z
			zint = Float64[]
		else
			e = max(10, maximum(abs.(extrema(z)))) + .2  # interpolation knots span a bit beyond [-10,10] to avoid edge effects; symmetric start at 0 if modelabsz=true
			kts = (modelabsz ? -.2 : -e) : 1/interpres : e  # LinRange(modelabsz ? 0 : -e, e, (2-modelabsz) * ceil(Int, e * interpres) + 1)
			zint = (z .- first(kts)) .* interpres .+ 1
		end

		Î©, WHermite = gausshermite(NHermite)
		Î© .= âˆš2 .* Î©; WHermite ./= âˆšÏ€  # fold in adjustment for change of variables from pdf(Normal(Ï‰)) to exp(-xÂ²)

		Zâ‚€, W = gausslegendre(NLegendre)  # nodes and weights for Gauss-Legendre quadrature over [-1,1]
		Zâ‚€ .*= zÌ„; W .*= zÌ„  # change of variables to quadrature over [-zÌ„, zÌ„]
		
		new(modelabsz, [d], z, wt/mean(wt), length(z), length(kts), interpres!=0, kts, -zÌ„ .â‰¤ kts .â‰¤ zÌ„, splinetype, zint, NHermite, Î©, WHermite, log.(WHermite).+.5Î©.^2, NLegendre, Zâ‚€, W, log.(W), penalty, Dict(), Dict(), Dict(), Dict(), Dict())
	end
end

# to prevent "MethodError: ==(::ForwardDiff.Dual{ForwardDiff.Tag{var"#objective#178"{â€¦}, Float64}, Float64, 11}, ::IrrationalConstants.Invsqrt2) is ambiguous."
import Base.==
==(a::ForwardDiff.Dual, b::IrrationalConstants.Invsqrt2) = a == Float64(b)


#
# Hack'n'file log likelihood
#

# Compute observation-level likelihood (not log likelihood) and expected number of publish/file-drawer/p-hack decision junctures (G)
function _HnFll(M::HnFmodel; p::AbstractVector{T}, Î¼::AbstractVector{T}, Ï„::AbstractVector{T}, Î½::AbstractVector{T}, pDFHR::AbstractVector{T}, Ïƒ::Vector{T}, m::Vector{T}) where {T}
  pD, _, pH, pR = pDFHR
	lnpD, lnpH = log(pD), log(pH)
	lnpHÏƒm = lnpH + log(m[] / Ïƒ[])
	mm1 = m[] - 1

	zÌ„divÏƒ, zdivÏƒ, Zâ‚€divÏƒ = zÌ„/Ïƒ[], M.kts/Ïƒ[], M.Zâ‚€/Ïƒ[]

	is = findall(>(1e-6), p)  # nonzero mixture components
	_d = length(is)

	# pre-allocating these hampers automatic differentiation because they depend on T, which could be a Dual number
	âˆ« = _d<M.d[] ? Matrix{T}(undef,M.k,_d) : get!(M.âˆ«dict, T, Matrix{T}(undef,M.k,_d))  # likelihood contributions for each z knot and each mixture component
	G = zero(T)	 # accumulator for expected number of publish/file-drawer/p-hack decision junctures
	B = get!(M.Bdict, T, Vector{T}(undef, M.NLegendre))
	F = get!(M.Fdict, T, Matrix{T}(undef, M.NLegendre, M.k))  # Ï•(z;z_0,Ïƒ^2 ) ã€–Î”Î¦(|z|,-|z|;z_0,Ïƒ^2 )ã€—^(m-1) for each z and each zâ‚€ (Legendre integration point)
  tot_hacking = get!(M.tot_hacking_dict, T, Vector{T}(undef,M.k))

  if pH < eps()
    E = M.lnWLegendre
  else
    E = get!(M.Edict, T, Vector{T}(undef,M.NLegendre))  # w/(1-p_H  Î”Î¦(z Ì…,-z Ì…;z_0,Ïƒ^2 ) ) for each zâ‚€ (Legendre integration point)
    for k âˆˆ eachindex(E)  # for each Legendre point; pre-compute part of p-hacking contribution
      @inbounds E[k] = M.lnWLegendre[k] - log1mexp(lnpH + m[] * logdiffcdf(ğ’©, Zâ‚€divÏƒ[k]+zÌ„divÏƒ, Zâ‚€divÏƒ[k]-zÌ„divÏƒ))  # w/(1-p_H  Î”Î¦(z Ì…,-z Ì…;z_0,Ïƒ^2 ) )
    end
  end

	Threads.@threads for j âˆˆ 1:M.k
		@inbounds begin
			b = zdivÏƒ[j]; absb = abs(b)
			M.modelabsz && (neg2b = -2b)

			M.insig[j] && (tot_hacking[j] = log(pD + (pH < eps() ? pR : pR / exp(log1mexp(lnpH + logdiffcdf(ğ’©, b+zÌ„divÏƒ, b-zÌ„divÏƒ) * m[])))))

			l = LinearIndices(F)[1,j]  # index of top entry in this col, arrays being stored col-first
			for k âˆˆ eachindex(Zâ‚€divÏƒ)  # for each zâ‚€ (Legendre integration point)
				a = Zâ‚€divÏƒ[k]
				if a+absb â‰‰ a-absb
					Fâ‚–â±¼ = mm1 * logdiffcdf(ğ’©, a+absb, a-absb) - .5(log2Ï€ + (a-b)^2)
					M.modelabsz && (Fâ‚–â±¼ += log1pexp(neg2b * a))  # log [Ï•(a-b) + Ï•(a+b)] = log[Ï•(a-b)] + log[1+exp(-2ab)]
					F[l] = Fâ‚–â±¼
				else
					F[l] = -floatmax()  # z->0 limit if m â‰¥ 1
				end
				l += 1
			end
		end
	end

	@inbounds for _i âˆˆ 1:_d
		i = is[_i]

		# f(z_0) for ith mixture component, integrating out Ï‰ with Gauss-Hermite quadrature
		# because this is an inner loop, economize by manually computing the log t pdf while avoiding redundant work
		Ï„áµ¢Â² = Ï„[i]^2; _Ï„áµ¢Â² = 1+1/Ï„áµ¢Â²; sqrt_Ï„áµ¢Â² = âˆš_Ï„áµ¢Â²
		halfinv_Ï„áµ¢Â² = .5 / _Ï„áµ¢Â²
		_Î½áµ¢ = Î½[i]/2 + .5
		D = (1 + Ï„[i]^2) * Î½[i]
		Cáµ¢ = log(p[i]) - logbeta(Î½[i]/2,.5) - .5log(D)  # contains constant factor in t pdf, in logs
		lnf_zâ‚€_i(zâ‚€) = logsumexp(begin  # log [âˆ«_(-âˆ)^âˆ Ï•(zâ‚€;Ï‰)t(Ï‰;Î¼,Ï„áµ¢Â²,Î½áµ¢)dÏ‰] sans ln Cáµ¢ factor
												d = (zâ‚€ - Î¼[]) / sqrt_Ï„áµ¢Â²
												lnwpxÂ² - halfinv_Ï„áµ¢Â² * (x - d / Ï„áµ¢Â²)^2 - log1p((x + d)^2 / D) * _Î½áµ¢
											end
											for (x,lnwpxÂ²) âˆˆ zip(M.Î©, M.lnWpÎ©Â²))

    Gáµ¢ = zero(T)
    for k âˆˆ eachindex(E)	# for each zâ‚€ (Legendre integration point)
      t = E[k] + lnf_zâ‚€_i(M.Zâ‚€[k])
      Gáµ¢ += exp(t)
		  B[k] = lnpHÏƒm + t
    end
    G += exp(Cáµ¢) * Gáµ¢

		Threads.@threads for j âˆˆ 1:M.k  # for each z value/interpolation point
			@inbounds begin
				lnf_zâ‚€áµ¢â±¼ = M.modelabsz ? logsumexp(lnf_zâ‚€_i(M.kts[j]), lnf_zâ‚€_i(-M.kts[j])) : lnf_zâ‚€_i(M.kts[j])
				if pH < eps()  # special case of pH=0
					âˆ«â±¼ = M.insig[j] ? lnf_zâ‚€áµ¢â±¼ + tot_hacking[j] : lnf_zâ‚€áµ¢â±¼
				else
					if M.insig[j]  # component from using or reverting to initial measurement
						if pD < eps()  # special case of pD=0
							âˆ«â±¼ = lnf_zâ‚€áµ¢â±¼ + tot_hacking[j]
						else
							âˆ«â±¼ = lnpD + logsumexp(F[k,j] + B[k] for k âˆˆ eachindex(B))  # p-hacking contribution, integrating out zâ‚€
							âˆ«â±¼ = logsumexp(âˆ«â±¼, lnf_zâ‚€áµ¢â±¼ + tot_hacking[j])
						end
					else
						âˆ«â±¼ = logsumexp(F[k,j] + B[k] for k âˆˆ eachindex(B))  # p-hacking contribution, integrating out zâ‚€
						âˆ«â±¼ = logsumexp(âˆ«â±¼, lnf_zâ‚€áµ¢â±¼)
					end
				end
				âˆ«[j,_i] = Cáµ¢ + âˆ«â±¼
			end
		end
	end
  logsumexp!(tot_hacking, âˆ«), G  # sum across mixture components, into `tot_hacking` because it's the right size and already allocated
end

function HnFll(M::HnFmodel; pDFHR, kwargs...)
	âˆ«, G = _HnFll(M; pDFHR, kwargs...)
	M.interpolate && (âˆ« = interpolate!(âˆ«, BSpline(Cubic())).(M.zint))
	â„’ = (iszero(length(M.wt)) ? ThreadsX.sum(âˆ«) : dot(M.wt,âˆ«)) - xlog1py(M.N, -pDFHR[2]*G) + M.penalty(; pDFHR, kwargs...)
end


# simulate hack'n'file data generating process with integer m
# returns named tuple of true z's (Ï‰), initial measurements (zâœ»), and reported results
# NaN = file-drawered
# if truncate=true (the default), restricts all return results to published studies
function HnFDGP(N::Int; p::Vector{Float64}, Î¼::Vector{Float64}=[0.], Ï„::Vector{Float64}, Î½::Vector{Float64}, pDFHR::Vector{Float64}, Ïƒ::Vector{Float64}, m ::Vector{Float64}, modelabsz::Bool=false, truncate::Bool=true)
	Ï‰ = Vector{Float64}(undef,N)
	zâ‚€ = similar(Ï‰)
	zâœ» = similar(Ï‰)
	TÎ¼Ï„Î½ = GenT.(Î¼, Ï„, Î½)
	Threads.@threads for j âˆˆ eachindex(Ï‰)
		@inbounds begin
			i = rand(Distributions.Categorical(p))
			Ï‰[j] = Ï‰â±¼ = rand(TÎ¼Ï„Î½[i])  # pick mixture component
			zâ‚€[j] = Ï‰â±¼ + rand(ğ’©)  # initial measurement, variance 1 around Ï‰
		end
	end

	pD, pF, _, pR = pDFHR
	pFD  = pF + pD
  pFDR = pF + pD + pR

	Threads.@threads for i âˆˆ eachindex(zâ‚€)  # for each simulated study
		@inbounds begin
			zâ‚€â±¼ = zâ‚€[i]
			if abs(zâ‚€â±¼) > zÌ„  # if initial result significant, publish as is
				zâœ»[i] = zâ‚€â±¼
			else
				r = rand()
				if r < pF  # file-drawer initial, insignificant result?
					zâœ»[i] = NaN
				elseif r<pFDR  # publish initial, insigicant result
					zâœ»[i] = zâ‚€â±¼
				else  # p-hack
					while true
						batch = rand(Normal(zâ‚€â±¼, Ïƒ[]), Int(m[]))  # m measurements
						záµ¢ = batch[findfirst(x->abs(x)==maximum(abs.(batch)), batch)]  # most significant of batch
						if abs(záµ¢) > zÌ„  # if significant, publish and stop
							zâœ»[i] = záµ¢
							break
						else
							r = rand()
							if r < pFDR  # after halting p-hacking search, file-drawer or publish latest, insignificant result, or revert to initial measurement
								zâœ»[i] = r<pF ? NaN : r<pFD ? záµ¢ : zâ‚€â±¼
								break
							end
						end
					end
				end
			end
			modelabsz && (zâœ»[i] = abs(zâœ»[i]))
		end
	end

	if truncate
		keep = @. !isnan(zâœ») # && abs(zâœ»)<10
		Ï‰, zâ‚€, zâœ»  = Ï‰[keep], zâ‚€[keep], zâœ»[keep]
	end
	(Ï‰=Ï‰, zâ‚€=zâ‚€, zâœ»=zâœ»)
end

@kwdef struct HnFresult<:RegressionModel
	estname::String
	modelabsz::Bool
	converged::Bool
	coefdict::NamedTuple
	coefnames::Vector{String}
	coef::Vector{Float64}
	vcov::Matrix{Float64}
	k::Int
	n::Int
	d::Int  # number of mixture components possibly net of deletion of trivial ones
	ll::Float64
	BIC::Float64 =  k*log(n)-2ll
	se::Vector{Float64} = sqrt0.(diag(vcov))
	z::Vector{Float64} = coef ./ se
	ğ’©::Vector{Union{Missing, Normal{Float64}}} = [isnan(s) ? missing : Normal(c,s) for (c,s) âˆˆ zip(coef,se)]
end


#
# Setup to report HnFresult's with RegressionTables.jl. A lot of work!
#
begin
	# StatsAPI.aic( R::HnFresult) = 2 * (R.k âˆ’ R.ll)
	# StatsAPI.aicc(R::HnFresult) = 2 * (R.k + R.k * (R.k âˆ’ 1) / (R.n âˆ’ R.k âˆ’ 1) âˆ’ R.ll)
	StatsAPI.bic( R::HnFresult) = R.k * log(R.n) âˆ’ 2R.ll
	StatsAPI.coef(R::HnFresult) = R.coef
	StatsAPI.coefnames(R::HnFresult) = R.coefnames
	# StatsAPI.confint(R::HnFresult; level::Real=0.95) = [quantile.(R.ğ’©, (1-level)/2) cquantile.(R.ğ’©, (1-level)/2)]
	# StatsAPI.coeftable(R::HnFresult; level::Real=0.95) = (CI = confint(R; level);
	# 					                                             CoefTable([R.coef, 
	# 																											            R.se, 
	# 																																  R.z,
	# 																																  2ccdf.(ğ’©, abs.(R.z)), 
	# 																																  eachcol(CI)...],
	# 																											           ["Estimate", "Std.Error", "z value", "Pr(>|z|)", "Lower 95%", "Upper 95%"],
	# 																											           R.coefnames,
	# 																											           4,
	# 																											           3))
	StatsAPI.dof(R::HnFresult) = R.k
	# StatsAPI.informationmatrix(R::HnFresult; expected::Bool = true) = 
	StatsAPI.isfitted(R::HnFresult) = true
	StatsAPI.islinear(R::HnFresult) = false
	# StatsAPI.loglikelihood(model::HnFresult, observation) = 
	StatsAPI.loglikelihood(R::HnFresult) = R.ll
	StatsAPI.nobs(R::HnFresult) = R.n
	StatsAPI.vcov(R::HnFresult) = R.vcov
	StatsAPI.weights(R::HnFresult) = UnitWeights(R.n)
	StatsAPI.dof_residual(R::HnFresult) = R.n - R.k
	# StatsAPI.fitted(R::HnFresult) = 
	StatsAPI.responsename(R::HnFresult) = R.estname
	# StatsModels.formula(R::HnFresult) = Term(R.estname) ~ sum(Term.(R.coefnames))

	RegressionTables._responsename(x::HnFresult) = StatsAPI.responsename(x)
	RegressionTables._coefnames(x::HnFresult) = coefnames(x)
	RegressionTables.default_print_control_indicator(x::AbstractRenderType) = false

	struct Converged <: RegressionTables.AbstractRegressionStatistic val::Union{Bool, Nothing} end
	Converged(m::HnFresult) = Converged(m.converged)
	RegressionTables.label(render::AbstractRenderType, x::Type{Converged}) = "Converged"

	Base.repr(render::AbstractRenderType, x::LogLikelihood; args...) = format(RegressionTables.value(x); commas=true, precision=0) # https://github.com/jmboehm/RegressionTables.jl/issues/160#issuecomment-2139998831
	Base.repr(render::AbstractRenderType, x::BIC; args...) = format(RegressionTables.value(x); commas=true, precision=0) # https://github.com/jmboehm/RegressionTables.jl/issues/160#issuecomment-2139998831
	Base.repr(render::AbstractRenderType, x::Converged; args...) = RegressionTables.value(x) ? "Yes" : "No"
end


# set up and fit model
# any extra keyword arguments are passed to Optim.Options
function HnFfit(z::Vector, wt::Vector=Float64[]; d=1, interpres=0, NLegendre=50, NHermite=50, from::NamedTuple=NamedTuple(), xform::NamedTuple=NamedTuple(),
									methods::Vector=[NewtonTrustRegion()], estname="", modelabsz::Bool=false, penalty::Function=(; kwargs...)->0., kwargs...)

	println("\nModeling $estname data with $d mixture component(s)")
	
	# set starting values & parameter transformes, allowing caller to override defaults
	from  = merge((p=fill(1/d,d), Î¼=[0.]     , Ï„=collect(LinRange(1,d,d)), Î½=fill(1.,d), pDFHR=fill(.25,4), Ïƒ=[1.]      , m=[2.]        ),  from)
  xform = merge((p=SimplextoRâ¿, Î¼=identity , Ï„=bcast(log)              , Î½=bcast(log), pDFHR=SimplextoRâ¿, Ïƒ=bcast(log), m=bcast(log1m)), xform)

	M = HnFmodel(z, wt; d, modelabsz, interpres, NLegendre, NHermite, penalty)
	
	_from = pairs(from)
	fromxform = [xform[p](v) for (p,v) âˆˆ _from]  # starting values in optimization parameter space

	# indexes to extract individual parameter vectors from full parameter vector
	extractor = zip(keys(_from), Iterators.accumulate((ind,f)->f isa Number ? (last(ind)+1) : last(ind)+1:last(ind)+length(f), fromxform, init=0))

	xformer(x) = (p=>inverse(xform[p])(x[e]) for (p,e) âˆˆ extractor)  # map primary parameters into full model space, expressed as functions of optimization parameters, e.g. exp(log(Ïƒ))
	objective(x) = -HnFll(M; xformer(x)...)
	Î¸ = vcat(fromxform...)

	res = nothing
	for method âˆˆ methods
		res = Optim.optimize(objective, Î¸, method, Optim.Options(; merge((iterations=250, show_trace=true), kwargs)...), autodiff=:forward)
		Î¸ = Optim.minimizer(res)
	end

	invxform = Î¸ -> [Î¸[e] |> inverse(xform[p]) for (p,e) âˆˆ extractor]
	coefdict_maker(v) = NamedTuple(p=>inverse(xform[p])(v[e]) for (p,e) âˆˆ extractor)
	coefdict = coefdict_maker(Î¸)

	function derived_stats(; p,Î¼,Ï„,Î½,pDFHR,Ïƒ,m)
		pD, pF, pH, pR = pDFHR

		f(v) = ((Ï‰,zâ‚€)=v; pdf(ğ’©,zâ‚€ - Ï‰) * p'pdf.(GenT.(Î¼,Ï„,Î½), Ï‰))  # f(zâ‚€)
		g(v) = ((_,zâ‚€)=v; f(v) / (1 - pH * diffcdf(Normal(zâ‚€,Ïƒ[]),zÌ„,-zÌ„)))  # f * "shots on goal"
		Iâ‚€   = hcubature(f, [-100,-zÌ„], [100, zÌ„])[1] 
		Sâ‚‚â‚„  = hcubature(f, [-100,-4], [100,-2])[1] + hcubature(f, [-100,2], [100,4])[1]  # actually marginally significant
		G    = hcubature(g, [-100,-zÌ„], [100, zÌ„])[1] 
		Shâ‚‚â‚„ = pH * hcubature(v -> ((Ï‰,zâ‚€)=v; g(v) * (diffcdf(Normal(zâ‚€,Ïƒ[]),4,-4)^m[] - diffcdf(Normal(zâ‚€,Ïƒ[]),2,-2)^m[])), [-100,-zÌ„], [100,zÌ„])[1]  # p-hacked "marginally significant"

		[
			pF*G / Iâ‚€                     # fraction of insignificant studies file-drawered
			pF*G                          # fraction of all studies file-drawered
			pR*G / Iâ‚€ + pD                # fraction of insignificant published as is
			1 - (1-pH)*G/Iâ‚€               # fraction of initially insignificant that lead to published, significant, p-hacked results
			pD * (G/Iâ‚€ - 1)               # fraction of initially insignificant that lead to published, insignificant, p-hacked results

			(Iâ‚€ - (1-pH)*G) / (1 - pF*G)  # fraction of significant results that are p-hacked
			pD * (G - Iâ‚€) / (1 - pF*G)    # fraction of insignificant results that are p-hacked
			Shâ‚‚â‚„ / (Shâ‚‚â‚„ + Sâ‚‚â‚„)           # p-hacked fraction of "marginally significant" in Star Wars (2<|z|<4)
		]
	end

	Î” = ForwardDiff.jacobian(v->vcat(invxform(v)..., derived_stats(;coefdict_maker(v)...)), Î¸)  # Jacobian of full model parameters & derived stats wrt optimization parameters
	H = ForwardDiff.hessian(objective, Î¸)  # Hessian of log likelihood wrt optimization parameters
	Vxform = try pinv(H) catch _ fill(NaN, size(H)) end  # covariance matrix of optimization parameters
	vcov = Î” * Vxform * Î”'  # covariance matrix of full model parameters
	vcov[diagind(vcov)] .= max.(0, vcov[diagind(vcov)])

	# se = NamedTuple([p=> iszero(length(e)) ? zeros(length(inverse(xform[p])(Î¸[e]))) :
	# 											(e isa Int ? ForwardDiff.derivative : ForwardDiff.jacobian)(inverse(xform[p]), Î¸[e]) |>
	# 												(Î”áµ¢ -> Î”áµ¢ isa Number ? sqrt0(Vxform[e,e])*abs(Î”áµ¢) : sqrt0.(diag(Î”áµ¢ * Vxform[e,e] * Î”áµ¢')))
	# 									for (p,e) âˆˆ extractor])

	converged = Optim.converged(res)

	t = findall(x->abs(x)>.001, coefdict[:p])  # non-trivial mixture components
	if length(t) < d
		println("Dropping mixture components with negligible weight: keeping $(length(t)) of $d components")
		coefdict = (p=coefdict.p[t], Î¼=coefdict.Î¼, Ï„=coefdict.Ï„[t], Î½=coefdict.Î½[t], pDFHR=coefdict.pDFHR, Ïƒ=coefdict.Ïƒ, m=coefdict.m)
		I = vcat(t, 1+d, t.+(1+d), t.+(1+2d), 2+3d:size(vcov,1))  # indexes of kept parameters in full parameter vector
		vcov = vcov[I,I]
		M.d[] = d = length(t)
	end

	one2D = first(Unicode.graphemes("â‚â‚‚â‚ƒâ‚„"),d)
	coefnames = vcat("p".*one2D, "Î¼", "Ï„".*one2D, "Î½".*one2D, "pD", "pF", "pH", "pR", "Ïƒ", "m", "frac_insig_file_drawered", "overall_file_drawer_frac", 
										 "frac_insig_pubbed_as_is", "sig_p_hacked_frac", "insig_p_hacked_frac",
										 "p_hacked_frac_of_pubbed_insig", "p_hacked_frac_of_sig", "p_hacked_frac_of_marg_sig")

	G = _HnFll(M; coefdict...)[2]
	HnFresult(; estname, modelabsz, converged, coefdict, coefnames, coef=vcat(coefdict..., derived_stats(;coefdict...)...), vcov, k=length(Î¸), n=size(z,1), d, ll=-Optim.minimum(res))
end


function HnFplot(z, est, wt::Vector=Float64[]; NLegendre=50, NHermite=50, zplot::StepRangeLen=-5+1e-3:.01:5, Ï‰plot::StepRangeLen=zplot, title::String="")
	t = est.coefdict
	kwargsÏ‰ = (p=t.p, Î¼=t.Î¼, Ï„=t.Ï„, Î½=t.Î½)
	kwargsz = (pDFHR=t.pDFHR, Ïƒ=t.Ïƒ, m=t.m)
	kwargsz0 = (pDFHR=[1.,0.,0.,0.], Ïƒ=[1.], m=[1.])  # no distortion

	f = Figure(size=(1500,900))

	# empirical distribution of z's + model fit
	Axis(f[1,1], xlabel="z", ylabel="Density", limits=(est.modelabsz ? 0 : -10, 10, nothing,nothing))
	hist!(z, normalization=:pdf, bins=floor(Int,âˆšsize(z,1)), weights=length(wt)==0 ? Makie.automatic : wt, 
	        label="Actual published effects", color=(:slategray,.4))  # outline histogram of data

	s,e = extrema(z); _zplot = s:.01:e

  pplottrue                     = map(z->dot(t.p, pdf.(GenT.(kwargsÏ‰.Î¼, t.Ï„, t.Î½),  z)), _zplot)
  est.modelabsz && (pplottrue .+= map(z->dot(t.p, pdf.(GenT.(kwargsÏ‰.Î¼, t.Ï„, t.Î½), -z)), _zplot))
	pplottrue ./= 1 - est.coef[findfirst(==("overall_file_drawer_frac"), est.coefnames)]

	pplotinitial = fZ(_zplot; kwargsÏ‰..., kwargsz0..., modelabsz=est.modelabsz, NLegendre, NHermite)
	pplotfit     = fZ(_zplot; kwargsÏ‰..., kwargsz ..., modelabsz=est.modelabsz, NLegendre, NHermite)

	lines!(_zplot, pplottrue, label="Model: true effects", color=Makie.wong_colors()[3])
	lines!(_zplot, pplotinitial, label="Model: initial estimates", color=Makie.wong_colors()[1])
	lines!(_zplot, pplotfit, label="Model: published estimates", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)

	# distribution of z | Ï‰=2
	Ï‰ = 2.
	Axis(f[1,2], xlabel="Reported z | true z = $Ï‰", ylabel="Density")
	lines!(zplot, fZcondÎ©.(zplot, Ï‰; kwargsz0..., NLegendre), label="updating from prior")
	lines!(zplot, fZcondÎ©.(zplot, Ï‰; kwargsz..., NLegendre), label="updating from prior + research distortion", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)
	
	# distribution of Ï‰ | z=2
	_z = 2.
	Axis(f[2,1], xlabel="True z | reported z = $_z", ylabel="Density")
	lines!(Ï‰plot, fÎ©condZ.(Ï‰plot, _z; kwargsÏ‰..., kwargsz0..., NLegendre, NHermite), label="updating from prior")
	lines!(Ï‰plot, fÎ©condZ.(Ï‰plot, _z; kwargsÏ‰..., kwargsz..., NLegendre, NHermite), label="updating from prior + research distortion", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)
	
	# frequentist equal-tailed CI's as fn of z--Andrews & Kasy (2014), Figure 2
	CIs0 = Cquant.([.025 .5 .975], zplot; rtol=.0001, kwargsz0..., NLegendre)
	CIs  = Cquant.([.025 .5 .975], zplot; rtol=.0001, kwargsz..., NLegendre )
	Axis(f[1,3], xlabel="Reported z", ylabel="Point estimate and 95% CI for true z", xticks=-5:5, yticks=-6:6)
	lines!(zplot, CIs0[:,1], color=Makie.wong_colors()[1], label="No adjustment")
	lines!(zplot, CIs0[:,2], color=Makie.wong_colors()[1], linestyle=:dash)
	lines!(zplot, CIs0[:,3], color=Makie.wong_colors()[1])
	lines!(zplot, CIs[:,1], color=Makie.wong_colors()[6], label="Adjusting for research distortion")
	lines!(zplot, CIs[:,2], color=Makie.wong_colors()[6], linestyle=:dash)
	lines!(zplot, CIs[:,3], color=Makie.wong_colors()[6])
	try
		lb = linear_interpolation(CIs[:,1],zplot)(0.)  # McCrary, Christensen, and Fanelli (2016)-style z thresholds for p<.05
		ub = linear_interpolation(CIs[:,3],zplot)(0.)
		scatter!([lb;ub],[0.;0], color=Makie.wong_colors()[6])
		text!(lb, 0., text=format("{:03.2f}", lb), align=(:right, :bottom), fontsize=18)
		text!(ub, 0., text=format("{:03.2f}", ub), align=(:left, :top), fontsize=18)
	catch e
	end
	axislegend(position=:lt, framevisible = false)

	# Posterior mean of Ï‰ as fn of Z
	pplot0 = EÎ©condZ(zplot; kwargsÏ‰..., kwargsz0..., NLegendre, NHermite)
	pplot  = EÎ©condZ(zplot; kwargsÏ‰..., kwargsz..., NLegendre, NHermite)
	Axis(f[2,2], xlabel="Reported z", ylabel="Expected true z")
	lines!(zplot, zplot, label="As is", color=Makie.wong_colors()[3])
	lines!(zplot, pplot0, label="Updating from prior", color=Makie.wong_colors()[1])
	lines!(zplot, pplot , label="updating from prior + research distortion", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)

	# E[Ï‰] discount
	Axis(f[2,3], xlabel="Reported z", ylabel="Discount multiplier" #=, yticks=0:.1:1.5 , limits=(nothing,nothing,0.,nothing)=#)
	lines!(zplot[zplot.>.2], Float16.(pplot0[zplot.>.2]./zplot[zplot.>.2]), label="updating from prior")  # https://discourse.julialang.org/t/range-step-cannot-be-zero/66948/11?u=droodman
	lines!(zplot[zplot.>.2], Float16.(pplot[zplot.>.2]./zplot[zplot.>.2]), label="updating from prior + research distortion", color=Makie.wong_colors()[6])
  y = EÎ©condZ([2]; kwargsÏ‰..., kwargsz0..., NLegendre, NHermite)[] / 2
  scatter!(2, y, color=Makie.wong_colors()[1])
	text!(2, y, text=format("{:03.2f}", y), align=(:center, :bottom), fontsize=18)
  y = EÎ©condZ([2]; kwargsÏ‰..., kwargsz... , NLegendre, NHermite)[] / 2
  scatter!(2, y, color=Makie.wong_colors()[6])
	text!(2, y, text=format("{:03.2f}", y), align=(:center, :top), fontsize=18)
  axislegend(position=:lt, framevisible = false)

	title=="" || (f[0, 1:3] = Label(f, title))
	f |> display
	save("output/$(est.estname) all.png", f)

	fAK = Figure(size=(1000,500))
	fAK[0, 1:2] = Label(fAK, title)
	Axis(fAK[1,1], xlabel="True z", ylabel="Median bias in reported z")
	lines!(Ï‰plot, zeros(size(Ï‰plot)))
	lines!(Ï‰plot, quantFcondÎ©.(.5, Ï‰plot; kwargsz..., NLegendre) .- Ï‰plot)

	Axis(fAK[1,2], xlabel="True z", ylabel="Coverage of reported 95% CI")
	lines!(Ï‰plot, fill(.95, size(Ï‰plot)...))
	lines!(Ï‰plot, @. FZcondÎ©(Ï‰plot+zÌ„, Ï‰plot; kwargsz..., NLegendre)-FZcondÎ©(Ï‰plot-zÌ„, Ï‰plot; kwargsz..., NLegendre))
	fAK |> display
	save("output/$(est.estname) A&K Fig1.png", fAK)
end


#
# confirm match between model and simulation
#

p = [.7,.3]
Î¼ = [0.7]
Ï„ = [1.2,2.7]
Î½ = [20., 20.]
pD = .4
pF = .3
pH = .2
pR = .1
Ïƒ = [.2]
m = [5.]
d = length(p)
modelabsz = false
pDFHR = [pD, pF, pH, pR]
kwargs = (p=p, Î¼=Î¼, Ï„=Ï„, Î½=Î½, pDFHR=pDFHR, Ïƒ=Ïƒ, m=m, modelabsz=modelabsz)

n = 100_000
Random.seed!(1232)
sim = HnFDGP(n; kwargs...)

f = Figure()
Axis(f[1,1], limits=(modelabsz ? 0 : -10, 10, nothing,nothing))
hist!(sim.zâœ»[abs.(sim.zâœ»).<100], bins=10*2*100, normalization=:pdf)
zplot = (modelabsz ? 0 : -10):.01:10
lines!(zplot, fZ(zplot; NHermite=50, kwargs...), color=:orange, label="True parameters")
f|>display

penalty(; m::Vector{T}, Ï„::Vector{T}, Ïƒ::Vector{T}, kwargs...) where {T} = logpdf(Normal(0,5), log(m[])) + logpdf(Normal(0,5), log(Ïƒ[])) + sum(logpdf(Normal(0,5), log(Ï„áµ¢)) for Ï„áµ¢ âˆˆ Ï„) 
res = HnFfit(sim.zâœ»; d, modelabsz, penalty, estname="simulated", extended_trace=false)  # penalized maximum likelihood
print(res.coefdict)
lines!(zplot, fZ(zplot; modelabsz, res.coefdict...)[:,1], color=:green, label="Estimated parameters")

f[0, :] = Label(f, "Simulation vs model")
axislegend(position=:lt, framevisible=false)
colsize!(f.layout, 1, Relative(1))
f |> display

#
# model real data
#

@time begin
	# penalty function for parameters that can generate singularities
  penalty(; m::Vector{T}, Ï„::Vector{T}, Ïƒ::Vector{T}, kwargs...) where {T} = logpdf(Normal(0,5), log(m[])) + logpdf(Normal(0,5), log(Ïƒ[])) + sum(logpdf(Normal(0,5), log(Ï„áµ¢)) for Ï„áµ¢ âˆˆ Ï„) 

	# van Zwet, Schwab, and Senn (2021) data, https://osf.io/xq4b2
	df = DataFrame(CSV.File("data/van Zwet, Schwab, and Senn 2021/CochraneEffects.csv"))
	@. @subset!(df, abs(:z)<10 && :"outcome.nr"==1 && :RCT=="yes" && :"outcome.group"=="efficacy")  # vZSS used 20 not 10
	Random.seed!(29384)
	df = combine(groupby(df, :"study.id.sha1"), :z => sample => :z)  # randomly choose among primary outcomes
  results = [HnFfit(df.z; d, penalty, NLegendre=50, estname="vZZS$d") for d âˆˆ 1:3]
	vZSS = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, vZSS; title="van Zwet, Schwab, and Senn (2021) data")

	# Schuemie et al. (2013), https://onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2Fsim.5925&file=Appendix+G+Revision.xlsx
	df = DataFrame(XLSX.readtable("data/Schuemie et al. 2013/appendix g revision.xlsx", "NeatTable", first_row=2, infer_eltypes=true)...)
	@. df.z = log(df."Effect estimate") / (log(df."Upper bound of 95% CI" / df."Lower bound of 95% CI") / 2zÌ„)
	@. @subset!(df, abs(:z)<10)
	disallowmissing!(df, :z)
	results = [HnFfit(df.z; d, penalty, estname="Setal$d") for d âˆˆ 1:3]
	Setal = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, Setal; title="Schuemie et al. (2013) data")

	# Star Wars, doi.org/10.1257/app.20150044, openicpsr.org/openicpsr/project/113633/version/V1/view?path=/openicpsr/113633/fcr:versions/V1/brodeur_le_sangnier_zylberberg_replication/Data/Final/final_stars_supp.dta&type=file
	df = DataFrame(CSV.File("data/Brodeur et al. 2016/final_stars_supp.csv"))
	df.z = df.coefficient_num ./ df.standard_deviation_num
	@. @subset!(df, lowercase(:main)=="yes" && !ismissing(df.z) && abs(df.z)<20)
	disallowmissing!(df, :z)
	results = [HnFfit(df.z, df.weight_table; d, penalty, estname="SW$d") for d âˆˆ 1:3]
	SW = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, SW, df.weight_table; title="Brodeur et al. (2016) data")

	# Brodeur, Cook, and Heyes 2020, DOI 10.1257/aer.20190687, openicpsr.org/openicpsr/project/120246/version/V1/view?path=/openicpsr/120246/fcr:versions/V1/MM-Data.dta&type=file
	df = DataFrame(CSV.File("data/Brodeur, Cook, and Heyes 2020/MM Data.csv"))
	df.z = df.mu ./ df.sd  # .* (2*rand(Bernoulli(.5),size(df,1)).-1)
	@. @subset!(df, !ismissing(:z) && !isnan(:z) && abs(:z)<10)
	disallowmissing!(df, :z)
	hist(df.z, bins=100) |> display
	df.z .= abs.(df.z)
	results = [HnFfit(df.z; d, penalty, xform=(Î¼=get0,), modelabsz=true, estname="BCH$d") for d âˆˆ 1:3]
	BCH = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, BCH; title="Brodeur, Cook, and Heyes (2020) data")

	# Arel-Bundock et al. 2026
	df = DataFrame(CSV.File("data/Arel-Bundock et al. 2026/arel-bundock_briggs.csv"))
	@. @subset!(df, !ismissing.(df.z_stat) .&& abs.(df.z_stat).<10)
	results = [HnFfit(df.z_stat; d, penalty, estname="ABetal$d") for d âˆˆ 1:3]
	ABetal = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	@time HnFplot(df.z_stat, ABetal; title="Arel-Bundock et al. (2026) data")

	# Vivalt 2020, DOI 10.1093/jeea/jvaa019, https://figshare.com/articles/dataset/Replication_files_for_How_Much_Can_We_Generalize_from_Impact_Evaluations_/12048600/1
	df = DataFrame(CSV.File("data/Vivalt 2020/data_unstandardized.csv"))
	df.z = df.treatmentcoefficient ./ df.treatmentstandarderror
	@. @subset!(df, abs(:z)<10)
	results = [HnFfit(df.z; d, penalty, estname="V$d") for d âˆˆ 1:3]
	V = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, V; title="Vivalt (2020) data")

	# Gerber and Malhotra 2008, https://www.nowpublishers.com/article/details/supplementary-info/100.00008024_supp.rar
	df = [DataFrame(load("data/Gerber and Malhotra 2008/AJPS_Data.xls", "All Studies"))[2:end,[:x4,:x6]] ;
				DataFrame(load("data/Gerber and Malhotra 2008/APSR_Data.xls", "All Studies"))[2:end,[:x4,:x6]] ]
	@. @subset!(df, !ismissing(:x4))
	df.z = Float64.(df.x6)
	@. @subset!(df, abs.(:z)<10)
	results = [HnFfit(df.z; d, penalty, estname="GM$d") for d âˆˆ 1:3]
	GM = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, GM; title="Gerber & Malhotra (2008) data")

	# Georgescu and Wren 2018 ~1M sample, doi:10.1093/bioinformatics/btx811, https://github.com/agbarnett/intervals/blob/master/data/Georgescu.Wren.RData
	df = DataFrame(RData.load("data/Georgescu and Wren 2018/Georgescu.Wren.RData")["complete"])
	@. df.ci_level[ismissing(df.ci_level) || df.ci_level==.0095 || df.ci_level==.05] = .95
	@. df.z = log(df.mean) / (ifelse(ismissing(df.lower) || iszero(df.lower), log(df.upper / df.mean), log(df.upper / df.lower) / 2) / cquantile(ğ’©, (1 - df.ci_level)/2))
	@. @subset!(df, !ismissing(:z) && !ismissing(:lower) && iszero(:mistake) && abs(:z)<10.)  # van Zwet & Cator Figure 1 stops at 10
	# @. @subset!(df, :source!="Abstract")
	results = [HnFfit(df.z;            d, penalty, interpres=1000          , estname="GW$d") for d âˆˆ 1:3]  # approximate fits by interpolating loglik over z
	results = [HnFfit(df.z; results[d].d, penalty, from=results[d].coefdict, estname="GW$d") for d âˆˆ 1:3]
	GW = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]  # BIC minimizer
	HnFplot(df.z, GW; title="Georgescu and Wren (2018) data")

	table = regtable(GW, Setal, GM, SW, BCH, ABetal, vZSS, V;
							estim_decoration = (coef,p)->coef,  # no stars
							regression_statistics = [Nobs #=, Converged, LogLikelihood, BIC=#],
							print_estimator_section = false,
							keep = ["pâ‚", "pâ‚‚", "pâ‚ƒ", "pâ‚„", "Î¼", "Ï„â‚", "Ï„â‚‚", "Ï„â‚ƒ", "Ï„â‚„", "Î½â‚", "Î½â‚‚", "Î½â‚ƒ", "Î½â‚„", "pF", "pH", "pD", "pR", "Ïƒ", "m", "frac_insig_file_drawered", "frac_insig_pubbed_as_is", "p_hacked_frac_of_pubbed_insig", "p_hacked_frac_of_sig", "p_hacked_frac_of_marg_sig"],
							estimformat = "%0.3g",
							statisticformat = "%0.3g",
							number_regressions = false,
							file = "output/results.txt")
end
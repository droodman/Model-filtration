cd(dirname(@__FILE__))
cd("..")

using Pkg
Pkg.activate(".")  # activate this project's environment
Pkg.instantiate()  # make sure all packages installed

using Random, IrrationalConstants, Format, Distributions, Interpolations, Base.Iterators, FastGaussQuadrature, Optim, LogExpFunctions, CSV, DataFrames, DataFramesMeta, ForwardDiff, LinearAlgebra, Roots, QuadGK, Statistics, 
       InverseFunctions, StatsAPI, StatsBase, StatsModels, RegressionTables, Unicode, CairoMakie, Makie, ExcelFiles, XLSX, RData

const zÌ„ = 1.9599639845401
const ğ’© = Normal()

@inline diffcdf(N,b,a) = cdf(N,b) - cdf(N,a)
@inline sqrt0(x) = x<0 ? zero(typeof(x)) : sqrt(x)


# to parameterize an n-vector of probabilities summing to 1 with an unbounded (n-1)-vector, apply logistic transform to latter, then map to squared spherical coordinates
# https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates, https://math.stackexchange.com/questions/2861449/parameterizations-of-the-unit-simplex-in-mathbbr3
function Râ¿toSimplex(q::AbstractVector{T}) where {T}
	p = Vector{T}(undef, length(q)+1)
	Î sinÂ² = one(T)
	@inbounds for i âˆˆ eachindex(q)
		sinÂ², cosÂ² = (q[i] |> logistic |> sincospi).^2
		p[i] = Î sinÂ² * cosÂ²
		Î sinÂ² *= sinÂ²
	end
	p[end] = Î sinÂ²
	replace!(p, NaN=>0)
end
function SimplextoRâ¿(p::AbstractVector{T}) where {T}
	q = Vector{T}(undef, length(p)-1)
	sum = p[end]
	@inbounds for i âˆˆ reverse(eachindex(q))
		sum += p[i]
		q[i] = acos(âˆš(p[i] / sum)) / Ï€
	end
	q .= logit.(q)
end
InverseFunctions.inverse(::typeof(SimplextoRâ¿)) = Râ¿toSimplex

# transform to constrain parameters
get0(::Vector{T}) where {T} = T[]
put0(::Vector{T}) where {T} = T[0]  # constant 1
InverseFunctions.inverse(::typeof(get0)) = put0
get1(::Vector{T}) where {T} = T[]
put1(::Vector{T}) where {T} = T[1]  # constant 1
InverseFunctions.inverse(::typeof(get1)) = put1
get1000(::Vector{T}) where {T} = T[]
put1000(::Vector{T}) where {T} = T[1,0,0,0]  # constant 1,0,0,0
InverseFunctions.inverse(::typeof(get1000)) = put1000

# # transform to constrain pDFHR to have pR=0
# get_pR0(v::Vector{T}) where {T} = v[1:3]
# put_pR0(v::Vector{T}) where {T} = T[v; 0]
# InverseFunctions.inverse(::typeof(get_pR0)) = put_pR0
# get_pF0(v::Vector{T}) where {T} = [v[1]; v[3:4]]
# put_pF0(v::Vector{T}) where {T} = [v[1]; 0; v[2:3]]
# InverseFunctions.inverse(::typeof(get_pF0)) = put_pF0
# get_pHR0(v::Vector{T}) where {T} = v[1:2]
# put_pHR0(v::Vector{T}) where {T} = T[v; 0; 0]
# InverseFunctions.inverse(::typeof(get_pHR0)) = put_pHR0
# get_pDR0(v::Vector{T}) where {T} = v[2:3]
# put_pDR0(v::Vector{T}) where {T} = T[0; v; 0]
# InverseFunctions.inverse(::typeof(get_pDR0)) = put_pDR0

# functions to map x <-> fill(x,k) for k=1,2,3,4
shared1(x) = [x[1]]
shared2(x) = [x[1]]
shared3(x) = [x[1]]
shared4(x) = [x[1]]
const shared = shared1, shared2, shared3, shared4
fill1(x) = fill(x[],1)
fill2(x) = fill(x[],2)
fill3(x) = fill(x[],3)
fill4(x) = fill(x[],4)
InverseFunctions.inverse(::typeof(shared1)) = fill1
InverseFunctions.inverse(::typeof(shared2)) = fill2
InverseFunctions.inverse(::typeof(shared3)) = fill3
InverseFunctions.inverse(::typeof(shared4)) = fill4

log1m(x) = log(x - 1)
expp1(x) = exp(x) + 1
InverseFunctions.inverse(::typeof(log1m)) = expp1

bcast = Broadcast.BroadcastFunction  # short-hand for forming the broadcasting version of a function, which works with InverseFunctions


# compute f(z|Ï‰) & F(file drawer|Ï‰). Return as tuple
function _fZcondÎ©!(y, z, Ï‰; modelabsz::Bool=false, Nquad::Int=50, pDFHR::Vector{T}, Ïƒ::Vector{T}, m::Vector{T}) where {T}
	pD, _, pH, pR = pDFHR
  lnpH = log(pH)

	Zâ‚€, W = gausslegendre(Nquad)  # nodes and weights for Gauss-Legendre quadrature over [-1,1]
	Zâ‚€ .*= zÌ„  # change of variables to quadrature over [-zÌ„, zÌ„]
  lnW = log.(W) .+ log(zÌ„)

	zdivÏƒ, zÌ„divÏƒ = z/Ïƒ[], zÌ„/Ïƒ[]

  file_drawer = âˆ« = 0.
	b = zdivÏƒ; absb = abs(b)
	@inbounds for k âˆˆ 1:Nquad  # p-hacking; integrate out zâ‚€ over [-zÌ„, zÌ„]
		a = Zâ‚€[k] / Ïƒ[]
    B = lnW[k] + logpdf(ğ’©, Zâ‚€[k]-Ï‰) - log1mexp(lnpH + logdiffcdf(ğ’©, a+zÌ„divÏƒ, a-zÌ„divÏƒ) * m[])
    file_drawer += exp(B) 

		if a+absb â‰‰ a-absb
      F = logpdf(ğ’©, b-a) + logdiffcdf(ğ’©, a+absb, a-absb) * (m[]-1)
			modelabsz && (F += log1pexp(-2b * a))  # log [Ï•(a-b) + Ï•(a+b)] = log[Ï•(a-b)] + log[1+exp(-2ab)]
			âˆ« += exp(B + F)
		end
	end
	âˆ« *= m[] / Ïƒ[] * pH  # density contribution from p-hacking

	f_z = pdf(ğ’©, z-Ï‰)
	modelabsz && (f_z += pdf(ğ’©, z+Ï‰))

	âˆ« += f_z  # contribution from publishing original stat without p-hacking
	if -zÌ„ â‰¤ z â‰¤ zÌ„
		âˆ« *= pD  # in insignificant range, same formulas, but times pD
    âˆ« += pR * f_z / (1 - exp(lnpH + logdiffcdf(ğ’©, zdivÏƒ+zÌ„divÏƒ, zdivÏƒ-zÌ„divÏƒ) * m[]))  # contribution from reverting to original stat after p-hacking
	end
	y .= âˆ«, file_drawer
end

_fZcondÎ©(z, Ï‰; kwargs...) = _fZcondÎ©!(Vector{Float64}(undef,2), z, Ï‰; kwargs...)

 # f(z|Ï‰). If truncate=true (the default), returns the density conditional on publication
fZcondÎ©(z, Ï‰; modelabsz=false, Nquad=50, pDFHR, Ïƒ, m, truncate=true) = _fZcondÎ©(z, Ï‰; modelabsz, Nquad, pDFHR, Ïƒ, m) |> (y -> truncate ? y[1]/(1 - pDFHR[2]*y[2]) : y[1])
 
# likelihood for a collection (vector, step range) of z's for plotting
# If truncate=true (default), returns the truncated density, i.e., conditional on publication
function fZ(z; modelabsz=false, Nquad=50, p, Î¼, Ï„, pDFHR, Ïƒ, m, truncate=true)
  M = HnFmodel(z; d=length(Ï„), Nquad, modelabsz)
  âˆ«, file_drawer = _HnFll(M; p,Î¼,Ï„,pDFHR,Ïƒ,m)
  truncate && (âˆ« ./= 1 - file_drawer)
  âˆ«
end

# the most time-consuming plotting is of the confidence intervals: for various values of Ï‰, 
# the cdf F(z|Ï‰) is numerically calculated, many times--iteratively seeking where it hits, e.g., .025 and .975
# to save time, pre-compute all components of f(z|Ï‰) that do not depend on z, notably logdiffcdf(ğ’©(0,Ïƒ), Zâ‚€[k]+zÌ„, Zâ‚€[k]-zÌ„)
function FZcondÎ©(z, Ï‰; modelabsz::Bool=false, Nquad::Int=50, pDFHR, Ïƒ, m, rtol=.00001, order=13)
	pD, pF, pH, pR = pDFHR
  lnpH = log(pH)

	Zâ‚€, W = gausslegendre(Nquad)  # nodes and weights for Gauss-Legendre quadrature over [-1,1]
	Zâ‚€ .*= zÌ„  # change of variables to quadrature over [-zÌ„, zÌ„]
	W  .*= zÌ„
	
	zÌ„divÏƒ, Zâ‚€divÏƒ = zÌ„/Ïƒ[], Zâ‚€/Ïƒ[]

	A = 0.
	B = Vector{Float64}(undef, Nquad)
	@inbounds for k âˆˆ 1:Nquad
		a = Zâ‚€[k] / Ïƒ[]
		B[k] = log(W[k]) + logpdf(ğ’©, Zâ‚€[k] - Ï‰) - log1mexp(lnpH + logdiffcdf(ğ’©, a+zÌ„divÏƒ, a-zÌ„divÏƒ) * m[])
		A += exp(B[k])
	end

	function myfZcondÎ©(z)
		zdivÏƒ = z / Ïƒ[]
		b = abs(zdivÏƒ)

		âˆ« = 0.
		@inbounds for k âˆˆ 1:Nquad
			a = Zâ‚€divÏƒ[k]
			if a+b â‰‰ a-b
				Fâ‚– = -.5(zdivÏƒ - a)^2 + (m[]-1) * logdiffcdf(ğ’©, a+b, a-b)  # p_H Ï•(z;z_0,Ïƒ^2 )
				modelabsz && (Fâ‚– += log1pexp(-2a * b))  # log [Ï•(a-b) + Ï•(a+b)] = log[Ï•(a-b)] + log[1+exp(-2ab)]
				âˆ« += exp(B[k] + Fâ‚–)
			end
		end
		âˆ« *= pH / Ïƒ[] * m[]

		f_z = exp(-.5(z-Ï‰)^2)
		modelabsz && (f_z += exp(-.5(z+Ï‰)^2))

		âˆ« += f_z
		if -zÌ„ â‰¤ z â‰¤ zÌ„
			âˆ« *= pD
      âˆ« += pR * f_z / (1 - pH * diffcdf(ğ’©, zdivÏƒ+zÌ„divÏƒ, zdivÏƒ-zÌ„divÏƒ) ^ m[])
		end
		âˆ«
	end

	endpoints = modelabsz ? [0, zÌ„] : [-Inf, -zÌ„, zÌ„]  # since f(z|Ï‰) jumps at Â±zÌ„, do quadrature separately in each range
	endpoints = [endpoints[findall(<(z), endpoints)]; z]
	quadgk(myfZcondÎ©, endpoints...; rtol, order)[1] * invsqrt2Ï€ / (1 - pF * A)
end

quantFcondÎ©(q, Ï‰; kwargs...) = find_zero(z -> q - FZcondÎ©(z, Ï‰; kwargs...), (-20,20), Roots.ITP())  # ITP algorithm works well

# f(z), f(Ï‰), f(Ï‰|z), E[Ï‰|z]
# inconsistency: z should be a scalar for fÎ©condZ but a vector or other iterable for EÎ©condZ
fÎ©(Ï‰; p, Î¼, Ï„) = dot(p,pdf.(Normal.(Î¼,Ï„), Ï‰))
fÎ©condZ(Ï‰, z; p, Î¼, Ï„, kwargs...) = fZcondÎ©(z, Ï‰; kwargs..., truncate=false) * fÎ©(Ï‰; p, Î¼, Ï„) / fZ([z]; p, Î¼, Ï„, kwargs..., truncate=false)[]
EÎ©condZ(z; rtol=.00001, maxevals=1e4, p, Î¼, Ï„, kwargs...) = [quadgk(Ï‰ -> Ï‰ * fZcondÎ©(záµ¢, Ï‰; kwargs..., truncate=false) * fÎ©(Ï‰; p, Î¼, Ï„), -20, 20; rtol, maxevals)[1] for záµ¢âˆˆz] ./ 
                                                                      fZ(z; p, Î¼, Ï„, kwargs..., truncate=false)

# CIs
Cquant(Î±, z; kwargs...) = find_zero(Ï‰ -> Î± - FZcondÎ©(z, Ï‰; kwargs...), (-20,20), Roots.ITP())  # Andrews & Kasy (2019), eq. 2
CI(    Î±, z; kwargs...) = Cquant(Î±/2, z; kwargs...), Cquant(1-Î±/2, z; kwargs...)


# object to hold pre-computed stuff for hack'n'file log likelihood computation
# e.g., logit âˆ˜ shared[4] extracts the first of a quartet of model parameters and applies logit; inverse applies logistic and fills out a quartet
struct HnFmodel
	modelabsz::Bool  # modeling |z|?
	d::Int  # number of mixture components
	z::Vector{Float64}  # all data
	N::Int  # number of z's in data, # of insignificant
	k::Int  # number of z knots for interpolation
	interpolate::Bool	# interpolation resolution (points per unit interval); 0 means no interpolation
	kts::Vector{Float64}  # interpolation knots in z space
	insig::BitVector  # which knots are in insignificant region
	splinetype::Interpolations.InterpolationType  # type of interpolation
	zint::Vector{Float64}  # z values mapped to cardinal knot numbering space since interpolate() is faster with cardinally spaced knots
	Nquad::Int  # number of quadrature points
	Zâ‚€::Vector{Float64}; lnW::Vector{Float64}  # quadrature nodes & weights
  penalty::Function

	function HnFmodel(z; d::Int, modelabsz::Bool=false, interpres::Int=0, Nquad::Int=50, splinetype::Interpolations.InterpolationType=BSpline(Linear()), 
                    penalty::Function=(; kwargs...)->0.)
		if iszero(interpres)
			kts = z
			zint = Float64[]
		else
			e = max(10, maximum(abs.(extrema(z)))) + .2  # interpolation knots span a bit beyond [-10,10] to avoid edge effects; symmetric start at 0 if modelabsz=true
			kts = (modelabsz ? -.2 : -e) : 1/interpres : e  # LinRange(modelabsz ? 0 : -e, e, (2-modelabsz) * ceil(Int, e * interpres) + 1)
			zint = (z .- first(kts)) .* interpres .+ 1
		end

		Zâ‚€, W = gausslegendre(Nquad)  # nodes and weights for Gauss-Legendre quadrature over [-1,1]
		Zâ‚€ .*= zÌ„; W .*= zÌ„  # change of variables to quadrature over [-zÌ„, zÌ„]

		new(modelabsz, d, z, length(z), length(kts), interpres!=0, kts, -zÌ„ .â‰¤ kts .â‰¤ zÌ„, splinetype, zint, Nquad, Zâ‚€, log.(W), penalty)
	end
end

# to prevent "MethodError: ==(::ForwardDiff.Dual{ForwardDiff.Tag{var"#objective#178"{â€¦}, Float64}, Float64, 11}, ::IrrationalConstants.Invsqrt2) is ambiguous."
import Base.==
==(a::ForwardDiff.Dual, b::IrrationalConstants.Invsqrt2) = a == Float64(b)

#
# Hack'n'file log likelihood
#

# This core function does everything but taking logs of obs-level likelihoods, optionally interpolating, and dividing by 1-file-drawered mass
function _HnFll(M::HnFmodel; p::AbstractVector{T}, Î¼::AbstractVector{T}, Ï„::AbstractVector{T}, pDFHR::AbstractVector{T}, Ïƒ::Vector{T}, m::Vector{T}) where {T}
  pD, pF, pH, pR = pDFHR
	zÌ„divÏƒ, zdivÏƒ, Zâ‚€divÏƒ = zÌ„/Ïƒ[], M.kts/Ïƒ[], M.Zâ‚€/Ïƒ[]

	# pre-allocating these hampers automatic differentiation because they depend on T, which could be a Dual number
	âˆ« = zeros(T,M.k)
	file_drawer = zero(T)	# file-drawer mass accumulator
	B = Vector{T}(undef,M.Nquad)  # pre-multiplied by p_H for later use in B calculation
  tot_hacking = Vector{T}(undef,M.k)

  if iszero(pH)
    E = M.lnW
  else
    E = Vector{T}(undef,M.Nquad)  # w/(1-p_H  Î”Î¦(z Ì…,-z Ì…;z_0,Ïƒ^2 ) ) for each zâ‚€ (Legendre integration point)
    lnpH = log(pH)
    Threads.@threads for k âˆˆ 1:M.Nquad  # doing this math in logs seems to prevent NaNs in auto-differentiation
      @inbounds E[k] = M.lnW[k] - log1mexp(lnpH + m[] * log(diffcdf(ğ’©, Zâ‚€divÏƒ[k]+zÌ„divÏƒ, Zâ‚€divÏƒ[k]-zÌ„divÏƒ)))  # w/(1-p_H  Î”Î¦(z Ì…,-z Ì…;z_0,Ïƒ^2 ) )
    end
  end

	F = Matrix{T}(undef, M.Nquad, M.k)  # Ï•(z;z_0,Ïƒ^2 ) ã€–Î”Î¦(|z|,-|z|;z_0,Ïƒ^2 )ã€—^(m-1) for each z and each zâ‚€ (Legendre integration point)
	mm1 = m[] - 1
	Threads.@threads for j âˆˆ 1:M.k
		b = zdivÏƒ[j]; absb = abs(b)
		M.modelabsz && (neg2b = -2b)

    tot_hacking[j] = pD + (iszero(pH) ? pR : pR / exp(log1mexp(lnpH + logdiffcdf(ğ’©, b+zÌ„divÏƒ, b-zÌ„divÏƒ) * m[])))

		l = LinearIndices(F)[1,j]  # index of top entry in this col, arrays being stored col-first
		@inbounds for k âˆˆ 1:M.Nquad
			a = Zâ‚€divÏƒ[k]
			if a+absb â‰‰ a-absb
				Fâ‚–â±¼ = logpdf(ğ’©, a-b) + mm1 * logdiffcdf(ğ’©, a+absb, a-absb)
				M.modelabsz && (Fâ‚–â±¼ += log1pexp(neg2b * a))  # log [Ï•(a-b) + Ï•(a+b)] = log[Ï•(a-b)] + log[1+exp(-2ab)]
				F[l] = exp(Fâ‚–â±¼)
			else
				F[l] = 0  # z->0 limit if m â‰¥ 1
			end
			l += 1
		end
	end

  pHÏƒm = pH / Ïƒ[] * m[]
	@inbounds for i âˆˆ 1:M.d
    ğ’©Î¼ = Normal(Î¼[i], âˆš(1+Ï„[i]^2))

    Pr_file_draweráµ¢ = zero(T)
    for k âˆˆ 1:M.Nquad
      t = exp(E[k] + logpdf(ğ’©Î¼, M.Zâ‚€[k])) 
      Pr_file_draweráµ¢ += t
		  B[k] = pHÏƒm * t
    end
    file_drawer += p[i] * Pr_file_draweráµ¢

		Threads.@threads for j âˆˆ 1:M.k  # for each z value/interpolation point
			@inbounds begin
				âˆ«â±¼ = zero(T)
				l = LinearIndices(F)[1,j]  # index of top entry in this col, arrays being stored col-first
				for k âˆˆ 1:M.Nquad  # p-hacking contribution, integrating out zâ‚€
					âˆ«â±¼ += F[l] * B[k]
					l += 1
				end

				t = M.modelabsz ? pdf(ğ’©Î¼, M.kts[j])+pdf(ğ’©Î¼, -M.kts[j]) : pdf(ğ’©Î¼, M.kts[j])
        if M.insig[j]  # component from using or reverting to initial measurement
					âˆ«â±¼ *= pD
          âˆ«â±¼ += t * tot_hacking[j]
				else
          âˆ«â±¼ += t
				end
				âˆ«[j] += p[i] * âˆ«â±¼
			end
		end
	end
  âˆ«, pF*file_drawer
end

function HnFll(M::HnFmodel; p::AbstractVector{T}, Î¼::AbstractVector{T}, Ï„::AbstractVector{T}, pDFHR::AbstractVector{T}, Ïƒ::Vector{T}, m::Vector{T}) where {T}
  âˆ«, file_drawer = _HnFll(M; p,Î¼,Ï„,pDFHR,Ïƒ,m)
	Threads.@threads for j âˆˆ 1:M.k
		@inbounds âˆ«[j] = log(âˆ«[j])
	end
	sum(M.interpolate ? interpolate!(âˆ«, BSpline(Cubic())).(M.zint) : âˆ«) - xlog1py(M.N, -file_drawer) + M.penalty(; p, Î¼, Ï„, pDFHR, Ïƒ, m)
end


# simulate hack'n'file data generating process with integer m
# returns named tuple of true z's (Ï‰), initial measurements (zâœ»), and reported results
# NaN = file-drawered
# if truncate=true (the default), restricts all return results to published studies
function HnFDGP(N::Int; p::Vector{Float64}, Î¼::Vector{Float64}=[0.], Ï„::Vector{Float64}, pDFHR::Vector{Float64}, Ïƒ::Vector{Float64}, m ::Vector{Float64}, modelabsz::Bool=false, truncate::Bool=true)
	Ï‰ = Vector{Float64}(undef,N)
	zâ‚€ = similar(Ï‰)
	zâœ» = similar(Ï‰)
	ğ’©Î¼Ï„ = Normal.(Î¼, Ï„)
	Threads.@threads for j âˆˆ eachindex(Ï‰)
		@inbounds begin
			Ï‰[j] = Ï‰â±¼ = rand(ğ’©Î¼Ï„[rand(Distributions.Categorical(p))])  # pick Gaussian mixture component
			zâ‚€[j] = Ï‰â±¼ + rand(ğ’©)  # initial measurement, variance 1 around Ï‰
		end
	end

	pD, pF, _, pR = pDFHR
	pFD  = pF + pD
  pFDR = pF + pD + pR

	Threads.@threads for i âˆˆ eachindex(zâ‚€)  # for each simulated study
		@inbounds begin
			zâ‚€â±¼ = zâ‚€[i]
			if abs(zâ‚€â±¼) > zÌ„  # if initial result significant, publish as is
				zâœ»[i] = zâ‚€â±¼
			else
				r = rand()
				if r < pF  # file-drawer initial, insignificant result?
					zâœ»[i] = NaN
				elseif r<pFDR  # publish initial, insigicant result
					zâœ»[i] = zâ‚€â±¼
				else  # p-hack
					while true
						batch = rand(Normal(zâ‚€â±¼, Ïƒ[]), Int(m[]))  # m measurements
						záµ¢ = batch[findfirst(x->abs(x)==maximum(abs.(batch)), batch)]  # most significant of batch
						if abs(záµ¢) > zÌ„  # if significant, publish and stop
							zâœ»[i] = záµ¢
							break
						else
							r = rand()
							if r < pFDR  # after halting p-hacking search, file-drawer or publish latest, insignificant result, or revert to initial measurement
								zâœ»[i] = r<pF ? NaN : r<pFD ? záµ¢ : zâ‚€â±¼
								break
							end
						end
					end
				end
			end
			modelabsz && (zâœ»[i] = abs(zâœ»[i]))
		end
	end

	if truncate
		keep = @. !isnan(zâœ») && abs(zâœ»)<10
		Ï‰, zâ‚€, zâœ»  = Ï‰[keep], zâ‚€[keep], zâœ»[keep]
	end
	(Ï‰=Ï‰, zâ‚€=zâ‚€, zâœ»=zâœ»)
end

struct HnFresult<:RegressionModel
	estname::String
	modelabsz::Bool
	converged::Bool
	coefdict::NamedTuple
	coefnames::Vector{String}
	coef::Vector{Float64}
	vcov::Matrix{Float64}
	k::Int
	n::Int
	ll::Float64
	BIC::Float64
	se::Vector{Float64}
	z::Vector{Float64}
	ğ’©::Vector{Union{Missing, Normal{Float64}}}
  file_drawer::Float64

	function HnFresult(estname, modelabsz, converged, b, coefnames, coef, vcov, k, n, ll, file_drawer)
	  se = sqrt0.(diag(vcov))
		view(vcov, diagind(vcov)) |> t -> t[t.<0] .= 0  # zero out negative diagonal entries
		new(estname, modelabsz, converged, b, coefnames, coef, vcov, k, n, ll, k*log(n)-2ll, se, coef ./ se, 
		    [isnan(s) ? missing : Normal(c,s) for (c,s) âˆˆ zip(coef,se)], file_drawer)
	end
end


#
# Setup to report HnFresult's with RegressionTables.jl. A lot of work!
#

# StatsAPI.aic( R::HnFresult) = 2 * (R.k âˆ’ R.ll)
# StatsAPI.aicc(R::HnFresult) = 2 * (R.k + R.k * (R.k âˆ’ 1) / (R.n âˆ’ R.k âˆ’ 1) âˆ’ R.ll)
StatsAPI.bic( R::HnFresult) = R.k * log(R.n) âˆ’ 2R.ll
StatsAPI.coef(R::HnFresult) = R.coef
StatsAPI.coefnames(R::HnFresult) = R.coefnames
# StatsAPI.confint(R::HnFresult; level::Real=0.95) = [quantile.(R.ğ’©, (1-level)/2) cquantile.(R.ğ’©, (1-level)/2)]
# StatsAPI.coeftable(R::HnFresult; level::Real=0.95) = (CI = confint(R; level);
# 					                                             CoefTable([R.coef, 
# 																											            R.se, 
# 																																  R.z,
# 																																  2ccdf.(ğ’©, abs.(R.z)), 
# 																																  eachcol(CI)...],
# 																											           ["Estimate", "Std.Error", "z value", "Pr(>|z|)", "Lower 95%", "Upper 95%"],
# 																											           R.coefnames,
# 																											           4,
# 																											           3))
StatsAPI.dof(R::HnFresult) = R.k
# StatsAPI.informationmatrix(R::HnFresult; expected::Bool = true) = 
StatsAPI.isfitted(R::HnFresult) = true
StatsAPI.islinear(R::HnFresult) = false
# StatsAPI.loglikelihood(model::HnFresult, observation) = 
StatsAPI.loglikelihood(R::HnFresult) = R.ll
StatsAPI.nobs(R::HnFresult) = R.n
StatsAPI.vcov(R::HnFresult) = R.vcov
StatsAPI.weights(R::HnFresult) = UnitWeights(R.n)
StatsAPI.dof_residual(R::HnFresult) = R.n - R.k
# StatsAPI.fitted(R::HnFresult) = 
StatsAPI.responsename(R::HnFresult) = R.estname
# StatsModels.formula(R::HnFresult) = Term(R.estname) ~ sum(Term.(R.coefnames))

RegressionTables._responsename(x::HnFresult) = StatsAPI.responsename(x)
RegressionTables._coefnames(x::HnFresult) = coefnames(x)
RegressionTables.default_print_control_indicator(x::AbstractRenderType) = false

struct Converged <: RegressionTables.AbstractRegressionStatistic
    val::Union{Bool, Nothing}
end
Converged(m::HnFresult) = Converged(m.converged)
RegressionTables.label(render::AbstractRenderType, x::Type{Converged}) = "Converged"

Base.repr(render::AbstractRenderType, x::LogLikelihood; args...) = format(RegressionTables.value(x); commas=true, precision=0) # https://github.com/jmboehm/RegressionTables.jl/issues/160#issuecomment-2139998831
Base.repr(render::AbstractRenderType, x::BIC; args...) = format(RegressionTables.value(x); commas=true, precision=0) # https://github.com/jmboehm/RegressionTables.jl/issues/160#issuecomment-2139998831
Base.repr(render::AbstractRenderType, x::Converged; args...) = RegressionTables.value(x) ? "Yes" : "No"


# set up and fit model
# any extra keyword arguments are passed to Optim.Options
function HnFfit(z::Vector; d::Int=1, interpres::Int=0, Nquad::Int=50, method::Optim.AbstractOptimizer=NewtonTrustRegion(), from::NamedTuple=NamedTuple(), xform::NamedTuple=NamedTuple(),
									estname="", modelabsz::Bool=false, penalty::Function=(; kwargs...)->0., kwargs...)

	# set starting values & parameter transformes, allowing caller to override defaults
	from  = merge((p=fill(1/d,d), Î¼=fill(0.,d), Ï„=collect(LinRange(1,d,d)), pDFHR=fill(.25,4), Ïƒ=[1.]      , m=[2.]        ),  from)
  xform = merge((p=SimplextoRâ¿, Î¼=shared[d] , Ï„=bcast(log)              , pDFHR=SimplextoRâ¿, Ïƒ=bcast(log), m=bcast(log1m)), xform)

	M = HnFmodel(z; d, modelabsz, interpres, Nquad, penalty)
	
	_from = pairs(from)
	fromxform = [xform[p](v) for (p,v) âˆˆ _from]  # starting values in optimization parameter space

	# indexes to extract scalar and vector parameters from full parameter vector
	extractor = zip(keys(_from), Iterators.accumulate((ind,f)->f isa Number ? (last(ind)+1) : last(ind)+1:last(ind)+length(f), fromxform, init=0))

	xformer(x) = (p=>inverse(xform[p])(x[e]) for (p,e) âˆˆ extractor)  # map primary parameters into full model space, expressed as functions of optimization parameters, e.g. exp(log(Ïƒ))
	objective(x) = -HnFll(M; xformer(x)...)

	res = Optim.optimize(objective, vcat(fromxform...), method, Optim.Options(; merge((iterations=100, show_trace=true), kwargs)...), autodiff=:forward)
	Î¸ = Optim.minimizer(res)
	invxform = Î¸ -> [Î¸[e] |> inverse(xform[p]) for (p,e) âˆˆ extractor]
	b = NamedTuple([p=>Î¸áµ¢ for ((p,e),Î¸áµ¢) âˆˆ zip(extractor,invxform(Î¸))])

	Î” = ForwardDiff.jacobian(v->vcat(invxform(v)...), Î¸)  # Jacobian of full model parameters wrt optimization parameters
	H = ForwardDiff.hessian(objective, Î¸)  # Hessian of log likelihood wrt optimization parameters
	Vxform = try pinv(H) catch _ fill(NaN, size(H)) end  # covariance matrix of optimization parameters
	V = Î” * Vxform * Î”'  # covariance matrix of full model parameters

	# se = NamedTuple([p=> iszero(length(e)) ? zeros(length(inverse(xform[p])(Î¸[e]))) :
	# 											(e isa Int ? ForwardDiff.derivative : ForwardDiff.jacobian)(inverse(xform[p]), Î¸[e]) |>
	# 												(Î”áµ¢ -> Î”áµ¢ isa Number ? sqrt0(Vxform[e,e])*abs(Î”áµ¢) : sqrt0.(diag(Î”áµ¢ * Vxform[e,e] * Î”áµ¢')))
	# 									for (p,e) âˆˆ extractor])

	converged = Optim.converged(res)

	one2D = first(Unicode.graphemes("â‚â‚‚â‚ƒâ‚„"),d)
	coefnames = vcat("p".*one2D, 
	                  modelabsz & false ? String[] : from.Î¼ isa Number ? "Î¼" : "Î¼".*one2D, 
									  "Ï„".*one2D, "pD", "pF", "pH", "pR", "Ïƒ", "m")

	ll = -Optim.minimum(res)
  file_drawer = _HnFll(M; b...)[2]
	HnFresult(estname, modelabsz, converged, b, coefnames, vcat(b...), V, length(Î¸), size(z,1), ll, file_drawer)
end

function HnFplot(z, est; zplot::StepRangeLen=-5+1e-3:.01:5, Ï‰plot::StepRangeLen=zplot, title::String="")
	t = est.coefdict
	kwargsÏ‰ = (p=t.p, Î¼=t.Î¼, Ï„=t.Ï„)
	kwargsz = (pDFHR=t.pDFHR, Ïƒ=t.Ïƒ, m=t.m)
	kwargsz0 = (pDFHR=[1.,0.,0.,0.], Ïƒ=[1.], m=[1.])  # no distortion

	f = Figure(size=(1500,900))

	# empirical distribution of z's + model fit
	CairoMakie.Axis(f[1,1], xlabel="z", ylabel="Density")
	hist!(z, normalization=:pdf, bins=floor(Int,âˆšsize(z,1)), label="Actual published effects", color=(:slategray,.4))  # outline histogram of data

	s,e = extrema(z); _zplot = s:.01:e
	published_mass = 1 - est.file_drawer

  pplottrue    = map(z->dot(t.p, (@. pdf(Normal(kwargsÏ‰.Î¼, t.Ï„       ), z))) / published_mass, _zplot)
	pplotinitial = map(z->dot(t.p, (@. pdf(Normal(kwargsÏ‰.Î¼, âˆš(t.Ï„^2+1)), z))) / published_mass, _zplot)
  if est.modelabsz
    pplottrue    .+= map(z->dot(t.p, (@. pdf(Normal(kwargsÏ‰.Î¼, t.Ï„       ), -z))) / published_mass, _zplot)
    pplotinitial .+= map(z->dot(t.p, (@. pdf(Normal(kwargsÏ‰.Î¼, âˆš(t.Ï„^2+1)), -z))) / published_mass, _zplot)
  end

	pplotfit = fZ(_zplot; kwargsÏ‰..., kwargsz..., modelabsz=est.modelabsz)
	lines!(_zplot, pplottrue, label="Model: true effects", color=Makie.wong_colors()[3])
	lines!(_zplot, pplotinitial, label="Model: initial estimates", color=Makie.wong_colors()[1])
	lines!(_zplot, pplotfit, label="Model: published estimates", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)

	# distribution of z | Ï‰=2
	Ï‰ = 2.
	Axis(f[1,2], xlabel="Reported z | true z = $Ï‰", ylabel="Density")
	lines!(zplot, fZcondÎ©.(zplot, Ï‰; kwargsz0...), label="updating from prior")
	lines!(zplot, fZcondÎ©.(zplot, Ï‰; kwargsz...), label="updating from prior + research distortion", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)
	
	# distribution of Ï‰ | z=2
	_z = 2.
	Axis(f[2,1], xlabel="True z | reported z = $_z", ylabel="Density")
	lines!(Ï‰plot, fÎ©condZ.(Ï‰plot, _z; kwargsÏ‰..., kwargsz0...), label="updating from prior")
	lines!(Ï‰plot, fÎ©condZ.(Ï‰plot, _z; kwargsÏ‰..., kwargsz...), label="updating from prior + research distortion", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)
	
	# frequentist equal-tailed CI's as fn of z--Andrews & Kasy (2014), Figure 2
	CIs0 = Cquant.([.025 .5 .975], zplot; rtol=.0001, Nquad=50, kwargsz0...)
	CIs  = Cquant.([.025 .5 .975], zplot; rtol=.0001, Nquad=50, kwargsz... )
	lb = linear_interpolation(CIs[:,1],zplot)(0.)  # McCrary, Christensen, and Fanelli (2016)-style z thresholds for p<.05
	ub = linear_interpolation(CIs[:,3],zplot)(0.)
	Axis(f[1,3], xlabel="Reported z", ylabel="Point estimate and 95% CI for true z", xticks=-5:5, yticks=-6:6)
	lines!(zplot, CIs0[:,1], color=Makie.wong_colors()[1], label="No adjustment")
	lines!(zplot, CIs0[:,2], color=Makie.wong_colors()[1], linestyle=:dash)
	lines!(zplot, CIs0[:,3], color=Makie.wong_colors()[1])
	lines!(zplot, CIs[:,1], color=Makie.wong_colors()[6], label="Adjusting for research distortion")
	lines!(zplot, CIs[:,2], color=Makie.wong_colors()[6], linestyle=:dash)
	lines!(zplot, CIs[:,3], color=Makie.wong_colors()[6])
	scatter!([lb;ub],[0.;0], color=Makie.wong_colors()[6])
	text!(lb, 0., text=format("{:03.2f}", lb), align=(:right, :bottom))
	text!(ub, 0., text=format("{:03.2f}", ub), align=(:left, :top))
	axislegend(position=:lt, framevisible = false)

	# Bayesian posterior mean of Ï‰ as fn of Z
	pplot0 = EÎ©condZ(zplot; kwargsÏ‰..., kwargsz0...)
	pplot  = EÎ©condZ(zplot; kwargsÏ‰..., kwargsz...)
	Axis(f[2,2], xlabel="Reported z", ylabel="Expected true z")
	lines!(zplot, zplot, label="As is", color=Makie.wong_colors()[3])
	lines!(zplot, pplot0, label="Updating from prior", color=Makie.wong_colors()[1])
	lines!(zplot, pplot , label="updating from prior + research distortion", color=Makie.wong_colors()[6])
	axislegend(position=:lt, framevisible = false)

	# E[Ï‰] discount
	Axis(f[2,3], xlabel="Reported z", ylabel="Discount multiplier" #=, yticks=0:.1:1.5 , limits=(nothing,nothing,0.,nothing)=#)
	lines!(zplot[zplot.>.2], Float16.(pplot0[zplot.>.2]./zplot[zplot.>.2]), label="updating from prior")  # https://discourse.julialang.org/t/range-step-cannot-be-zero/66948/11?u=droodman
	lines!(zplot[zplot.>.2], Float16.(pplot[zplot.>.2]./zplot[zplot.>.2]), label="updating from prior + research distortion", color=Makie.wong_colors()[6])
  y = EÎ©condZ([2]; kwargsÏ‰..., kwargsz0...)[] / 2
  scatter!(2, y, color=Makie.wong_colors()[1])
	text!(2, y, text=format("{:03.2f}", y), align=(:center, :bottom))
  y = EÎ©condZ([2]; kwargsÏ‰..., kwargsz... )[] / 2
  scatter!(2, y, color=Makie.wong_colors()[6])
	text!(2, y, text=format("{:03.2f}", y), align=(:center, :top))
  axislegend(position=:rt, framevisible = false)

	title=="" || (f[0, 1:3] = Label(f, title))
	f |> display
	save("output/$(est.estname) all.png", f)

	fAK = Figure(size=(1000,500))
	fAK[0, 1:2] = Label(fAK, title)
	Axis(fAK[1,1], xlabel="True z", ylabel="Median bias in reported z")
	lines!(Ï‰plot, zeros(size(Ï‰plot)))
	lines!(Ï‰plot, quantFcondÎ©.(.5, Ï‰plot; kwargsz...) .- Ï‰plot)

	Axis(fAK[1,2], xlabel="True z", ylabel="Coverage of reported 95% CI")
	lines!(Ï‰plot, fill(.95, size(Ï‰plot)...))
	lines!(Ï‰plot, FZcondÎ©.(Ï‰plot.+zÌ„, Ï‰plot; kwargsz...).-FZcondÎ©.(Ï‰plot.-zÌ„, Ï‰plot; kwargsz...))
	fAK |> display
	save("output/$(est.estname) A&K Fig1.png", fAK)
end


# confirm match between model and simulation
p = [.7,.3]
Î¼ = [0.,0.]
Ï„ = [1.2,1.7]
pD = .25
pF = .25
pH = .25
pR = .25
Ïƒ = [.2]
m = [5.]
d = length(p)
modelabsz=false
pDFHR=[pD, pF, pH, pR]
kwargs = (p=p, Î¼=Î¼, Ï„=Ï„, pDFHR=pDFHR, Ïƒ=Ïƒ, m=m)
zplot = collect((modelabsz ? 0 : -10):.01:10)
Random.seed!(1232)
sim = HnFDGP(1_000_00; kwargs..., modelabsz)
f = hist(sim.zâœ», bins=1000, normalization=:pdf)
lines!(zplot, fZ(zplot; kwargs..., modelabsz), color=:orange)
penalty(; m::Vector{T}, Ï„::Vector{T}, Ïƒ::Vector{T}, kwargs...) where {T} = logpdf(Normal(0,5), log(m[])) + logpdf(Normal(0,5), log(Ïƒ[])) + sum(logpdf(Normal(0,5), log(Ï„áµ¢)) for Ï„áµ¢ âˆˆ Ï„) 
res = HnFfit(sim.zâœ»; d, modelabsz, penalty, extended_trace=false);
print(res.coefdict)
lines!(zplot, fZ(zplot; modelabsz, res.coefdict...), color=:green)
f |> display

# M = HnFmodel(sim.zâœ»; d, modelabsz, p=SimplextoRâ¿, Î¼=shared[d], Ï„=bcast(log), pDFHR=SimplextoRâ¿, Ïƒ=bcast(log), m=bcast(log1m))
# HnFll(M, p, Î¼, Ï„, pDFHR, Ïƒ, m)

@time begin
  penalty(; m::Vector{T}, Ï„::Vector{T}, Ïƒ::Vector{T}, kwargs...) where {T} = logpdf(Normal(0,5), log(m[])) + logpdf(Normal(0,5), log(Ïƒ[])) + sum(logpdf(Normal(0,5), log(Ï„áµ¢)) for Ï„áµ¢ âˆˆ Ï„) 

	# Georgescu and Wren 2018 ~1M sample, doi:10.1093/bioinformatics/btx811, https://github.com/agbarnett/intervals/blob/master/data/Georgescu.Wren.RData
	df = DataFrame(RData.load("data/Georgescu and Wren 2018/Georgescu.Wren.RData")["complete"])
	@. df.ci_level[ismissing(df.ci_level) || df.ci_level==.0095 || df.ci_level==.05] = .95
	@. df.z = log(df.mean) / (ifelse(ismissing(df.lower) || iszero(df.lower), log(df.upper / df.mean), log(df.upper / df.lower) / 2) / cquantile(ğ’©, (1 - df.ci_level)/2))
	@. @subset!(df, !ismissing(:z) && !ismissing(:lower) && iszero(:mistake) && abs(:z) < 10.)  # van Zwet & Cator Figure 1 stops at 10
	# @. @subset!(df, :source!="Abstract")
	results = [HnFfit(df.z; d, penalty, interpres=1000,                         ) for d âˆˆ 1:3]
	results = [HnFfit(df.z; d, penalty, from=results[d].coefdict, estname="GW$d") for d âˆˆ 1:3]
	GW = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]  # BIC minimizer
	HnFplot(df.z, GW; title="Georgescu and Wren (2018) data")
	# GW2_R0 = HnFfit(df.z; d=2, from=(pDFHR=[1/3,1/3,1/3,0],), xform=(pDFHR = SimplextoRâ¿ âˆ˜ get_pR0,), estname="GW2", title="Georgescu and Wren (2018) data, 2-component prior, pR=0")	

	# Schuemie et al. (2013), https://onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2Fsim.5925&file=Appendix+G+Revision.xlsx
	df = DataFrame(XLSX.readtable("data/Schuemie et al. 2013/appendix g revision.xlsx", "NeatTable", first_row=2, infer_eltypes=true)...)
	@. df.z = log(df."Effect estimate") / (log(df."Upper bound of 95% CI" / df."Lower bound of 95% CI") / 2zÌ„)
	@. @subset!(df, abs(:z)<10)
	disallowmissing!(df, :z)
	results = [HnFfit(df.z; d, penalty, estname="Setal$d") for d âˆˆ 1:3]
	Setal = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, Setal; title="Schuemie et al. (2013) data")

	# van Zwet, Schwab, and Senn (2021) data, https://osf.io/xq4b2
	df = DataFrame(CSV.File("data/van Zwet, Schwab, and Senn 2021/CochraneEffects.csv"))
	@. @subset!(df, abs(:z)<10 && :"outcome.nr"==1 && :RCT=="yes" && :"outcome.group"=="efficacy")  # vZSS used 20 not 10
	Random.seed!(29384)
	df = combine(groupby(df, :"study.id.sha1"), :z => sample => :z)  # randomly choose among primary outcomes
  results = [HnFfit(df.z; d, penalty, estname="vZZS$d") for d âˆˆ 1:3]
	vZSS = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, vZSS; title="van Zwet, Schwab, and Senn (2021) data")

	# Star Wars, doi.org/10.1257/app.20150044, openicpsr.org/openicpsr/project/113633/version/V1/view?path=/openicpsr/113633/fcr:versions/V1/brodeur_le_sangnier_zylberberg_replication/Data/Final/final_stars_supp.dta&type=file
	df = DataFrame(CSV.File("data/Brodeur et al. 2016/final_stars_supp.csv"))
	df.z = df.coefficient_num ./ df.standard_deviation_num
	@. @subset!(df, lowercase(:main)=="yes" && !ismissing(df.z) && abs(df.z)<10)
	disallowmissing!(df, :z)
	results = [HnFfit(df.z; d, penalty, estname="SW$d") for d âˆˆ 1:3]
	SW = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, SW; title="Brodeur et al. (2016) data")

	# Brodeur, Cook, and Heyes 2020, DOI 10.1257/aer.20190687, openicpsr.org/openicpsr/project/120246/version/V1/view?path=/openicpsr/120246/fcr:versions/V1/MM-Data.dta&type=file
	df = DataFrame(CSV.File("data/Brodeur, Cook, and Heyes 2020/MM Data.csv"))
	df.z = df.mu ./ df.sd  # .* (2*rand(Bernoulli(.5),size(df,1)).-1)
	@. @subset!(df, !ismissing(:z) && !isnan(:z) && abs(:z)<10)
	disallowmissing!(df, :z)
	hist(df.z, bins=100) |> display
	df.z .= abs.(df.z)
	results = [HnFfit(df.z; d, penalty, modelabsz=true, estname="BCH$d") for d âˆˆ 1:3]
	BCH = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, BCH; title="Brodeur, Cook, and Heyes (2020) data")

	# Arel-Bundock et al. 2026
	df = DataFrame(CSV.File("data/Arel-Bundock et al. 2026/arel-bundock_briggs.csv"))
	@. @subset!(df, !ismissing.(df.z_stat) .&& abs.(df.z_stat).<10)
	results = [HnFfit(df.z_stat; d, penalty, estname="ABetal$d") for d âˆˆ 1:3]
	ABetal = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	@time HnFplot(df.z_stat, ABetal; title="Arel-Bundock et al. (2026) data")

	# Vivalt 2020, DOI 10.1093/jeea/jvaa019, https://figshare.com/articles/dataset/Replication_files_for_How_Much_Can_We_Generalize_from_Impact_Evaluations_/12048600/1
	df = DataFrame(CSV.File("data/Vivalt 2020/data_unstandardized.csv"))
	df.z = df.treatmentcoefficient ./ df.treatmentstandarderror
	@. @subset!(df, abs(:z)<10)
	results = [HnFfit(df.z; d, penalty, estname="V$d") for d âˆˆ 1:3]
	V = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, V; title="Vivalt (2020) data")

	# Gerber and Malhotra (2008), https://www.nowpublishers.com/article/details/supplementary-info/100.00008024_supp.rar
	df = [DataFrame(load("data/Gerber and Malhotra 2008/AJPS_Data.xls", "All Studies"))[2:end,[:x4,:x6]] ;
				DataFrame(load("data/Gerber and Malhotra 2008/APSR_Data.xls", "All Studies"))[2:end,[:x4,:x6]] ]
	@. @subset!(df, !ismissing(:x4))
	df.z = Float64.(df.x6)
	@. @subset!(df, abs.(:z)<10)
	results = [HnFfit(df.z; d, penalty, estname="GM$d") for d âˆˆ 1:3]
	GM = results[argmin(isnan(t.BIC) ? Inf : t.BIC for t âˆˆ results)]
	HnFplot(df.z, GM; title="Gerber & Malhotra (2008) data")

	regtable(GW, Setal, GM, SW, BCH, ABetal, vZSS, V;
							estim_decoration = (coef,p)->coef,  # no stars
							regression_statistics = [Nobs #=, Converged, LogLikelihood, BIC=#],
							print_estimator_section = false,
							keep = ["pâ‚", "pâ‚‚", "pâ‚ƒ", "pâ‚„", "Î¼â‚", "Ï„â‚", "Ï„â‚‚", "Ï„â‚ƒ", "Ï„â‚„", "pF", "pH", "pD", "pR", "Ïƒ", "m"],
							estimformat = "%0.3g",
							statisticformat = "%0.3g",
							number_regressions = false,
							file = "output/results.txt")
end
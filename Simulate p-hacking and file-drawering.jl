using Random, Distributions, Interpolations, Base.Iterators, FastGaussQuadrature, BenchmarkTools, Optim, LogExpFunctions, Plots, CSV, DataFrames, DataFramesMeta, ForwardDiff, LinearAlgebra, Roots, QuadGK, Statistics, ThreadsX, InverseFunctions, FileIO, Images, StatsAPI, StatsBase, RegressionTables

const ZÌ„ = 1.9599639845401

@inline diffcdf(N,b,a) = cdf(N,b) - cdf(N,a)

@inline sqrtNaN(x) = x<0 ? typeof(x)(NaN) : sqrt(x)

import Base.rand, Distributions.pdf, Distributions.logpdf, Distributions.cdf, Distributions.logcdf, Distributions.ccdf, Distributions.logccdf, Statistics.quantile

# type to represent distribution of maximum of m normal draws, Float64 only
struct MaxNormal <: Distribution{Univariate, Continuous}
	m::Float64  # number of draws over which to take max
	invm::Float64
	Î¼::Float64  # mean
	Ïƒ::Float64  # sd
	ğ’©::Normal{Float64}
	MaxNormal(m=1., Î¼=0., Ïƒ=1.) = new(m,1/m,Î¼,Ïƒ,Normal(Î¼,Ïƒ))
end
cdf(s::MaxNormal, x::Float64) = cdf(s.ğ’©,x) ^ s.m
logccdf(s::MaxNormal, x::Float64) = log1mexp(s.m * logcdf(s.ğ’©,x))
ccdf(s::MaxNormal, x::Float64) = exp(logccdf(s,x))
pdf(s::MaxNormal, x::Float64) = pdf(s.ğ’©,x) * s.m * cdf(s.ğ’©,x)^(s.m-1)
logpdf(s::MaxNormal, x::Float64) = logpdf(s.ğ’©,x) + log(s.m) + (s.m-1) * logcdf(s.ğ’©,x)
quantile(s::MaxNormal, x::Float64) = quantile(s.ğ’©, x^s.invm)
rand(rng::AbstractRNG, s::MaxNormal) = quantile(s.ğ’©, rand(rng)^s.invm)

struct MinNormal <: Distribution{Univariate, Continuous}
	m::Float64  # number of draws over which to take max
	invm::Float64
	Î¼::Float64  # mean
	Ïƒ::Float64  # sd
	ğ’©::Normal{Float64}
	MinNormal(m=1., Î¼=0., Ïƒ=1.) = new(m,1/m,Î¼,Ïƒ,Normal(Î¼,Ïƒ))
end
cdf(s::MinNormal, x::Float64) = exp(logcdf(s,x))
logcdf(s::MinNormal, x::Float64) = log1mexp(s.m * logccdf(s.ğ’©,x))
ccdf(s::MinNormal, x::Float64) = ccdf(s.ğ’©,x) ^ s.m
pdf(s::MinNormal, x::Float64) = pdf(s.ğ’©,x) * s.m * ccdf(s.ğ’©,x)^(s.m-1)
logpdf(s::MinNormal, x::Float64) = logpdf(s.ğ’©,x) + log(s.m) + (s.m-1) * logccdf(s.ğ’©,x)
quantile(s::MinNormal, x::Float64) = cquantile(s.ğ’©, (1-x)^s.invm)
rand(rng::AbstractRNG, s::MinNormal) = cquantile(s.ğ’©, rand(rng)^s.invm)

# to parameterize an n-vector of probabilities summing to 1 with an unbounded (n-1)-vector, apply logistic transform to latter, then map to squared spherical coordinates
# https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates, https://math.stackexchange.com/questions/2861449/parameterizations-of-the-unit-simplex-in-mathbbr3
function Râ¿toSimplex(q::AbstractVector{T}) where T
	_q = logistic.(q)
	p = Vector{T}(undef, length(q)+1)
	Î sinÂ² = one(T)
	for i âˆˆ eachindex(_q)
		cosÂ² = cospi(_q[i])^2
		p[i] = Î sinÂ² * cosÂ²
		Î sinÂ² *= one(T) - cosÂ²
	end
	p[end] = Î sinÂ²
	p
end
function SimplextoRâ¿(p::AbstractVector{T}) where T
	q = Vector{T}(undef, length(p)-1)
	sum = p[end]
	for i âˆˆ reverse(eachindex(q))
		sum += p[i]
		q[i] = acos(âˆš(p[i] / sum)) / Ï€
	end
	q .= logit.(q)
end
InverseFunctions.inverse(::typeof(SimplextoRâ¿)) = Râ¿toSimplex

get1(x) = x[1]
get2(x) = x[1]
get3(x) = x[1]
get4(x) = x[1]
const consvec = (get1, get2, get3, get4)
InverseFunctions.inverse(::typeof(get1)) = x->fill(x,1)
InverseFunctions.inverse(::typeof(get2)) = x->fill(x,2)
InverseFunctions.inverse(::typeof(get3)) = x->fill(x,3)
InverseFunctions.inverse(::typeof(get4)) = x->fill(x,4)

bcast(f) = Broadcast.BroadcastFunction(f)  # short-hand for forming the broadcasting version of a function, which works with InverseFunctions

# unlogged likelihood for a single observation. For graphs.
function HnFl(z; p::Vector, Î¼=0., Ï„::Vector, pFâ‚€, pHâ‚€, kH=[0.,0.], m=1, truncate=true, rtol=.01)
	iszero(length(Î¼)) && (Î¼ = 0.)
	Î¼ isa Number && (Î¼ = fill(Î¼,length(Ï„)))
	pLâ‚€,pUâ‚€ = pHâ‚€
	kL ,kU  = kH
	pL = pLâ‚€ * exp(-kL*(ZÌ„+z))
	pU = pUâ‚€ * exp(-kU*(ZÌ„-z))
	pF = pFâ‚€ * (1 - pL - pU)
	pD = 1 - pL - pU - pF
	ÏƒÂ² = 1 .+ Ï„.^2
	ğ’©  = Normal()
	ğ’©Î¼ = @. Normal(Î¼, âˆšÏƒÂ²)
	ğ’©Ï‰ = @. NormalCanon(z + Î¼/Ï„^2, 1 + 1/Ï„^2)
	if abs(z) â‰¥ ZÌ„
		result = 0.
		@inbounds for (páµ¢,ğ’©Î¼áµ¢,ğ’©Ï‰áµ¢) âˆˆ zip(p,ğ’©Î¼,ğ’©Ï‰)
			if z < 0.
				result += páµ¢ * pdf(ğ’©Î¼áµ¢, z) * (1 + m * pLâ‚€ * exp(kL*(kL/2-ZÌ„)) * quadgk(Ï‰ -> (a = pdf(ğ’©Ï‰áµ¢,Ï‰) * exp(-kL*Ï‰) * diffcdf(ğ’©, ZÌ„-Ï‰+kL, -ZÌ„-Ï‰+kL) * ccdf(ğ’©,z-Ï‰)^(m-1) / (1-ccdf(ğ’©, -ZÌ„-Ï‰)^m);
																																		                 isnan(a) || isinf(a) ? 0. : a), 
																                                               -20, 20; rtol)[1])
			else
				result += páµ¢ * pdf(ğ’©Î¼áµ¢, z) * (1 + m * pUâ‚€ * exp(kU*(kU/2-ZÌ„)) * quadgk(Ï‰ -> (a = pdf(ğ’©Ï‰áµ¢,Ï‰) * exp( kU*Ï‰) * diffcdf(ğ’©, ZÌ„-Ï‰-kU, -ZÌ„-Ï‰-kU) * cdf(ğ’©,z-Ï‰)^(m-1) / (1 - cdf(ğ’©,  ZÌ„-Ï‰)^m);
																																		                 isnan(a) || isinf(a) ? 0. : a), 
																                                               -20, 20; rtol)[1])
			end
		end
	else
		result = pD * dot(p, pdf.(ğ’©Î¼, z))
	end
	truncate && (result /= 1 - pFâ‚€ * dot(p, @. (diffcdf(ğ’©Î¼, ZÌ„, -ZÌ„) - pLâ‚€ * exp(kL*(ÏƒÂ²*kL/2-Î¼-ZÌ„)) * diffcdf(ğ’©Î¼,  ÏƒÂ²*kL+ZÌ„,  ÏƒÂ²*kL-ZÌ„)
	                                                                - pUâ‚€ * exp(kU*(ÏƒÂ²*kU/2+Î¼-ZÌ„)) * diffcdf(ğ’©Î¼, -ÏƒÂ²*kU+ZÌ„, -ÏƒÂ²*kU-ZÌ„)  )))
	result
end

# object to hold pre-computed stuff for log likelihood computation
struct HnFmodel
	D::Int  # number of mixture components
	z::Vector{Float64}  # all data
	zC::Vector{Float64}  # just the central, insignificant z's
	ZÌ„pzC::Vector{Float64}; ZÌ„mzC::Vector{Float64}  # ZÌ„ .+ zC, ZÌ„ .- zC
	N::Int; NC::Int; NL::Int; NU::Int  # number of z's, insigficant z's, lower significant z's, upper
	knots::LinRange  # interpolation knots in [ZÌ„,max]
	spline::Interpolations.InterpolationType  # type of interpolation
	zLint::Vector{Float64}; zUint::Vector{Float64}  # lower- & upper-tail significant z values mapped to cardinal knot numbering space since interpolate() is faster with cardinally spaced knots
	X::Vector{Float64}; W::Vector{Float64}  # quadrature nodes & weights
	lnW::Vector{Float64}
	xforms::Dict{Symbol, Function}
end
# constructor
function HnFmodel(z::Vector{Float64}; D::Int, interpres::Int=100, quadnodes::Int=25, kwargs...)
	zC = z[abs.(z) .< ZÌ„]

	s = ZÌ„ - 3/interpres; e = max(10,maximum(z))+.2
	knots = s : 1/interpres : e  # LinRange(s, e, ceil(Int, (e - s) * interpres) + 1)
	zLint = @. (-z[z â‰¤ -ZÌ„] - s) * interpres + 1  # map tail z values to knot numbering 1, 2, ... for ZÌ„-3/interpres, ZÌ„-2/interpres, ...
	zUint = @. ( z[z â‰¥  ZÌ„] - s) * interpres + 1

	X, W = gausshermite(quadnodes)
	W ./= âˆšÏ€

	HnFmodel(D, z, zC, ZÌ„.+zC, ZÌ„.-zC, length(z), length(zC), length(zLint), length(zUint), knots, BSpline(Linear()), zLint, zUint, X, W, log.(W), Dict(kwargs))
end

# bulk log probabilities as function of data & parameters, for estimation
function HnFll(M::HnFmodel; p::AbstractVector{<:Real}, Î¼=0., Ï„::AbstractVector{<:Real}, pFâ‚€::Real, pHâ‚€::AbstractVector, kH::AbstractVector, m::Real)
	T = eltype(p)
	pLâ‚€, pUâ‚€ = pHâ‚€
	kL , kU  = kH

	max(pLâ‚€+pUâ‚€*exp(-2*kU*ZÌ„), pUâ‚€+pLâ‚€*exp(-2*kL*ZÌ„)) > 1 && return(T(NaN))

	iszero(length(Î¼)) && (Î¼ = 0.)
	Î¼ isa Number && (Î¼ = fill(Î¼,length(Ï„)))

	mm1 = m - one(T)
	pDâ‚€ = one(T) - pFâ‚€
	pH = [m*pLâ‚€*exp(kL*(kL/2-ZÌ„)), m*pUâ‚€*exp(kU*(kU/2-ZÌ„))]

	LC = zeros(T, M.NC)  # likelihood for central/insignificant obs, left & right tails
	âˆ«    = Vector{T}(undef, length(M.knots))  # pre-allocating this hampers automatic differentiation since type changes
	bufL = zeros(T, length(M.knots))
	bufU = zeros(T, length(M.knots))

	ÏƒÂ² = 1 .+ (Ï„Â² = Ï„.^2)
	ğ’©  = Normal()
	ğ’©Î¼ = Normal.(Î¼, .âˆšÏƒÂ²)

	for (páµ¢,Î¼áµ¢,Ï„áµ¢Â²,Ïƒáµ¢Â²,ğ’©Î¼áµ¢) âˆˆ zip(p,Î¼,Ï„Â²,ÏƒÂ²,ğ’©Î¼)
		# math on integration and interpolation points, outside loops
		Î©  = M.X * âˆš(2Ï„áµ¢Â² / Ïƒáµ¢Â²)  # 1st-order component of change of variables from pdf(Normal(Ï‰)) to exp(-xÂ²) for Gauss-Hermite quadrature
		Î©L = -ZÌ„ .- Î©
		Î©U =  ZÌ„ .- Î©
		ğ’©Î© = Normal.(Î©)

		# lower tail
		kt1 = collect((Î¼áµ¢/Ï„áµ¢Â² .- M.knots) * -Ï„áµ¢Â²/Ïƒáµ¢Â²)  #    -(z+Î¼áµ¢â„Ï„áµ¢Â²)/(1+1â„Ï„áµ¢Â²) for z at interpolation points -- negated 0th-order component of change of variables for quadrature
		kt2 = collect(kt1 - M.knots                 )  # z - (z+Î¼áµ¢â„Ï„áµ¢Â²)/(1+1â„Ï„áµ¢Â²) for z at interpolation points
		fill!(âˆ«, zero(T))
		@inbounds Threads.@threads for j âˆˆ eachindex(M.knots)
			kt1j, kt2j = kt1[j], kt2[j]
			for (ğ’©Ï‰,Ï‰,Ï‰l,Ï‰u,lnw) âˆˆ zip(ğ’©Î©,Î©,Î©L,Î©U,M.lnW)  # quadrature integration
				âˆ«[j] += exp(lnw - kL * (Ï‰ - kt1j) + logdiffcdf(ğ’©, kt1j + Ï‰u + kL, kt1j + Ï‰l + kL) + mm1 * logccdf(ğ’©Ï‰, kt2j) - log1mexp(m * logccdf(ğ’©, kt1j + Ï‰l)))
			end
		end
		@. bufL += páµ¢ * pdf(ğ’©Î¼áµ¢, -M.knots) * (one(T) + pH[1] * âˆ«)

		# upper tail
		kt1 .= collect((Î¼áµ¢/Ï„áµ¢Â² .+ M.knots) * -Ï„áµ¢Â²/Ïƒáµ¢Â²)  #    -(z+Î¼áµ¢â„Ï„áµ¢Â²)/(1+1â„Ï„áµ¢Â²) for z at interpolation points
		kt2 .= collect(kt1 + M.knots                 )  # z - (z+Î¼áµ¢â„Ï„áµ¢Â²)/(1+1â„Ï„áµ¢Â²) for z at interpolation points
		fill!(âˆ«, zero(T))
		@inbounds Threads.@threads for j âˆˆ eachindex(M.knots)
			kt1j, kt2j = kt1[j], kt2[j]
			for (ğ’©Ï‰,Ï‰,Ï‰l,Ï‰u,lnw) âˆˆ zip(ğ’©Î©,Î©,Î©L,Î©U,M.lnW)  # quadrature integration
				âˆ«[j] += exp(lnw + kU * (Ï‰ - kt1j) + logdiffcdf(ğ’©, kt1j + Ï‰u - kU, kt1j + Ï‰l - kU) + mm1 * logcdf(ğ’©Ï‰, kt2j) - log1mexp(m * logcdf(ğ’©, kt1j + Ï‰u)))
			end
		end
		@. bufU += páµ¢ * pdf(ğ’©Î¼áµ¢, M.knots) * (one(T) + pH[2] * âˆ«)

		@. LC += páµ¢ * pdf(ğ’©Î¼áµ¢, M.zC)  # likelihoods for center/insignificant observations
	end

	@. bufL = log(bufL)
	@. bufU = log(bufU)
	llL = interpolate!(bufL, M.spline).(M.zLint)  # log likelihoods for lower tail
	llU = interpolate!(bufU, M.spline).(M.zUint)  # log likelihoods for upper tail

	ThreadsX.sum(llL) + ThreadsX.sum(llU) + ThreadsX.mapreduce(log, +, LC, init=zero(T)) +
		M.NC * log(pDâ‚€) + mapreduce((ZÌ„pz,ZÌ„mz)->log1p(- pLâ‚€ * exp(-kL * ZÌ„pz) - pUâ‚€ * exp(-kU * ZÌ„mz)), +, M.ZÌ„pzC, M.ZÌ„mzC, init=zero(T)) - 
    xlog1py(M.N, -pFâ‚€ * dot(p, @. diffcdf(ğ’©Î¼,ZÌ„,-ZÌ„) - pLâ‚€ * exp(kL*(ÏƒÂ²*kL/2-Î¼-ZÌ„)) * diffcdf(ğ’©Î¼, ZÌ„+ÏƒÂ²*kL, -ZÌ„+ÏƒÂ²*kL) -
		                                                  pUâ‚€ * exp(kU*(ÏƒÂ²*kU/2+Î¼-ZÌ„)) * diffcdf(ğ’©Î¼, ZÌ„-ÏƒÂ²*kU, -ZÌ„-ÏƒÂ²*kU)))
end

# f(z|Ï‰)
function fZcondÎ©(z, Ï‰; pFâ‚€, pHâ‚€, kH, m, truncate=true)
	pLâ‚€,pUâ‚€ = pHâ‚€
	kL ,kU  = kH
	pDâ‚€ = 1 - pFâ‚€
	result = abs(z) < ZÌ„ ? pdf(Normal(Ï‰),z) * pDâ‚€ * (1 - pLâ‚€ * exp(-kL*(ZÌ„+z)) - pUâ‚€ * exp(-kU*(ZÌ„-z))) :
							          pdf(Normal(Ï‰),z) + exp(z < 0 ? logpdf(MinNormal(m,Ï‰),z) - logcdf( MinNormal(m,Ï‰),-ZÌ„) + log(pLâ‚€) + kL*(kL/2-ZÌ„-Ï‰) + logdiffcdf(Normal(Ï‰-kL),ZÌ„,-ZÌ„) :
											                                 logpdf(MaxNormal(m,Ï‰),z) - logccdf(MaxNormal(m,Ï‰), ZÌ„) + log(pUâ‚€) + kU*(kU/2-ZÌ„+Ï‰) + logdiffcdf(Normal(Ï‰+kU),ZÌ„,-ZÌ„)  )
	truncate && (result /= (1 - pFâ‚€ * (diffcdf(Normal(Ï‰), ZÌ„,-ZÌ„) - pLâ‚€ * exp(kL*(kL/2-ZÌ„-Ï‰)) * diffcdf(Normal(Ï‰-kL),ZÌ„,-ZÌ„) - 
	                                                              pUâ‚€ * exp(kU*(kU/2-ZÌ„+Ï‰)) * diffcdf(Normal(Ï‰+kU),ZÌ„,-ZÌ„))))
	isnan(result) || isinf(result) ? 0. : result
end

# F(z|Ï‰)
function FZcondÎ©(z, Ï‰; pFâ‚€, pHâ‚€, kH, m)
	pDâ‚€ = 1 - pFâ‚€
	pLâ‚€,pUâ‚€ = pHâ‚€
	kL ,kU  = kH
	ğ’© = Normal(Ï‰)
	D = diffcdf(Normal(Ï‰), ZÌ„,-ZÌ„) - pLâ‚€ * exp(kL*(kL/2-ZÌ„-Ï‰)) * diffcdf(Normal(Ï‰-kL),ZÌ„,-ZÌ„) - 
	                               pUâ‚€ * exp(kU*(kU/2-ZÌ„+Ï‰)) * diffcdf(Normal(Ï‰+kU),ZÌ„,-ZÌ„)  # P[no p-hack]
	if z > ZÌ„  # tails
		ğ’©max = MaxNormal(m,Ï‰)
		result = 1 - (pUâ‚€ * exp(logccdf(ğ’©max,z) - logccdf(ğ’©max, ZÌ„) + kU*(kU/2-ZÌ„+Ï‰) + logdiffcdf(ğ’©, ZÌ„-kU, -ZÌ„-kU)) + ccdf(ğ’©,z)) / (1 - pFâ‚€ * D)
	else
		if z < -ZÌ„
			ğ’©min = MinNormal(m,Ï‰)
			result =    pLâ‚€ * exp(logcdf(ğ’©min, z) - logcdf(ğ’©min, -ZÌ„) + kL*(kL/2-ZÌ„-Ï‰) + logdiffcdf(ğ’©, kL+ZÌ„,  kL-ZÌ„)) + cdf(ğ’©,z)
		else
			result =    pLâ‚€ * exp(                                       kL*(kL/2-ZÌ„-Ï‰) + logdiffcdf(ğ’©, kL+ZÌ„, kL-ZÌ„)) + cdf(ğ’©,-ZÌ„) + 
			                pDâ‚€ * (diffcdf(Normal(Ï‰), z,-ZÌ„) - pLâ‚€ * exp(kL*(kL/2-ZÌ„-Ï‰)) * diffcdf(Normal(Ï‰-kL),z,-ZÌ„) - 
											                                  pUâ‚€ * exp(kU*(kU/2-ZÌ„+Ï‰)) * diffcdf(Normal(Ï‰+kU),z,-ZÌ„)  )
		end
		result /= 1 - pFâ‚€ * D
	end
	result
end

quantFcondÎ©(q, Ï‰; kwargs...) = find_zero(z -> q - FZcondÎ©(z, Ï‰; kwargs...), (-20,20))

# f(z), f(Ï‰), f(Ï‰|z), E[Ï‰|z]
fZ = HnFl
fÎ©(Ï‰; p, Î¼, Ï„) = p'pdf.(Normal.(Î¼,Ï„), Ï‰)
fÎ©condZ(Ï‰, z; p, Î¼, Ï„, kwargs...) = fZcondÎ©(z, Ï‰; kwargs..., truncate=false) * fÎ©(Ï‰; p, Î¼, Ï„) / fZ(z; p, Î¼, Ï„, kwargs..., truncate=false)
EÎ©condZ(   z; p, Î¼, Ï„, kwargs...) = quadgk(Ï‰ -> Ï‰ * fÎ©condZ(Ï‰, z; p, Î¼, Ï„, kwargs...), -Inf, Inf)[1]

# CIs
Cquant(Î±, z; kwargs...) = find_zero(Ï‰ -> Î± - FZcondÎ©(z, Ï‰; kwargs...), (-20,20))
CI(    Î±, z; kwargs...) = Cquant(Î±/2, z; kwargs...), Cquant(1-Î±/2, z; kwargs...)


function HnFDGP(N; p::Vector, Î¼=0., Ï„::Vector, pFâ‚€, pHâ‚€, kH=[0.,0.], m=1, truncate=true, Ï‰=NaN)
	Î¼ isa Number && (Î¼ = fill(Î¼,length(Ï„)))
	pLâ‚€,pUâ‚€ = pHâ‚€
	kL ,kU  = kH

	if isnan(Ï‰)
		I = rand(Categorical(p), N)
		Î© = map(i->rand(Normal(Î¼[i], Ï„[i])), I)
	else
		Î© = fill(Ï‰,N)
	end

	Zâœ» = rand.(Normal.(Î©))
	Z = similar(Zâœ»)
	@inbounds Threads.@threads for i âˆˆ eachindex(Zâœ»)
		Zâœ»áµ¢ = Zâœ»[i]
		if abs(Zâœ»áµ¢) > ZÌ„
			Z[i] = Zâœ»áµ¢  # publish significant result as is
		else
			pL = pLâ‚€ * exp(-kL*(ZÌ„+Zâœ»áµ¢))  # probability of hacking to lower tail
			pU = pUâ‚€ * exp(-kU*(ZÌ„-Zâœ»áµ¢))  # probability of hacking to upper tail
			pF = pFâ‚€ * (1 - pL - pU)
			pD = 1 - pL - pU - pF
			r = rand()
			if r < pD
				Z[i] = Zâœ»áµ¢  # publish insignificant result as is
			elseif r < pD + pF
				Z[i] = NaN  # file-drawer
			elseif r < pD + pF + pL
				ğ’© = MinNormal(m,Î©[i])
				Z[i] = quantile(ğ’©, rand(Uniform(0., cdf(ğ’©, -ZÌ„))))  # hack to lower tail
			else
				ğ’© = MaxNormal(m,Î©[i])
				Z[i] = quantile(ğ’©, rand(Uniform(cdf(ğ’©, ZÌ„), 1.)))  # hack to upper tail
			end
		end
	end

	if truncate
		keep = @. !isnan(Z) && abs(Z)<10
		Î©  = Î©[keep]
		Zâœ» = Zâœ»[keep]
		Z  = Z[keep]
	end
	(Î©=Î©, Zâœ»=Zâœ», Z=Z)  # named tuple with results
end

struct HnFResult<:RegressionModel
	depname::String
	coefnames::Vector{String}
	coef::Vector{Float64}
	vcov::Matrix{Float64}
	k::Int
	n::Int
	ll::Float64
	se::Vector{Float64}
	z::Vector{Float64}
	ğ’©::Vector{Normal{Float64}}
	HnFResult(depname, coefnames, coef, vcov, k, n, ll) = (se = sqrtNaN.(diag(vcov)); new(depname, coefnames, coef, vcov, k, n, ll, se, coef ./ se, Normal.(coef,se)))
end

StatsAPI.aic( R::HnFResult) = 2 * (R.k âˆ’ R.ll)
StatsAPI.aicc(R::HnFResult) = 2 * (R.k + R.k * (R.k âˆ’ 1) / (R.n âˆ’ R.k âˆ’ 1) âˆ’ R.ll)
StatsAPI.bic( R::HnFResult) = R.k * log(R.n) âˆ’ 2 * R.ll
StatsAPI.coef(R::HnFResult) = R.coef
StatsAPI.coefnames(R::HnFResult) = R.coefnames
StatsAPI.confint(R::HnFResult; level::Real=0.95) = [quantile.(R.ğ’©, (1-level)/2) cquantile.(R.ğ’©, (1-level)/2)]
StatsAPI.coeftable(R::HnFResult; level::Real=0.95) = (CI = confint(R; level);
                                             CoefTable([R.coef, 
																						            R.se, 
																											  R.z,
																											  2ccdf.(Normal(), abs.(R.z)), 
																											  eachcol(CI)...],
																						           ["Estimate", "Std.Error", "z value", "Pr(>|z|)", "Lower 95%", "Upper 95%"],
																						           R.coefnames,
																						           4,
																						           3))
StatsAPI.dof(R::HnFResult) = R.k
# StatsAPI.informationmatrix(R::HnFResult; expected::Bool = true) = 
StatsAPI.isfitted(R::HnFResult) = true
StatsAPI.islinear(R::HnFResult) = false
# StatsAPI.loglikelihood(model::HnFResult, observation) = 
StatsAPI.loglikelihood(R::HnFResult) = R.ll
StatsAPI.nobs(R::HnFResult) = R.n
StatsAPI.vcov(R::HnFResult) = R.vcov
StatsAPI.weights(R::HnFResult) = UnitWeights(R.n)
StatsAPI.dof_residual(R::HnFResult) = R.n - R.k
StatsAPI.fitted(R::HnFResult) = R.coef
StatsAPI.responsename(R::HnFResult) = R.depname

function fit(M::HnFmodel, from::Union{AbstractDict,NamedTuple}; method::Optim.AbstractOptimizer=Newton())
	_from = from isa AbstractDict ? from : pairs(from)
	fromxform = [M.xforms[p](v) for (p,v) âˆˆ _from]

	# indexes to extract scalar and vector parameters from full parameter vector
	extractor = zip(keys(_from), Iterators.accumulate((ind,f)->f isa Number ? (last(ind)+1) : last(ind)+1:last(ind)+length(f), fromxform, init=0))

	objective(v) = -HnFll(M; (p => inverse(M.xforms[p])(v[e]) for (p,e) âˆˆ extractor)...)
	res = optimize(objective, vcat(fromxform...), method, autodiff=:forward)
	Î¸ = Optim.minimizer(res)

	invxform = Î¸ -> [Î¸[e] |> inverse(M.xforms[p]) for (p,e) âˆˆ extractor]
	b = NamedTuple([p=>Î¸áµ¢ for ((p,_),Î¸áµ¢) âˆˆ zip(extractor,invxform(Î¸))])

	# use delta method to get se's for untransformed parameters
	Î” = ForwardDiff.jacobian(v->vcat(invxform(v)...), Î¸)
	H = ForwardDiff.hessian(objective, Î¸)
	Vxform = try pinv(H) catch _ fill(NaN, size(H)) end
	sexform = sqrtNaN.(diag(Vxform))
	V = Î” * Vxform * Î”'

	se = NamedTuple([p=> (Î”áµ¢ = (e isa Int ? ForwardDiff.derivative : ForwardDiff.jacobian)(inverse(M.xforms[p]), Î¸[e]);
	                      Î”áµ¢ isa Number ? sqrtNaN(Vxform[e,e])*abs(Î”áµ¢) : sqrtNaN.(diag(Î”áµ¢ * Vxform[e,e] * Î”áµ¢')))
				           for (p,e) âˆˆ extractor])

	(res=res, Î”=Î”, H=H, Vxform=Vxform, sexform=sexform, V=V, se=se, b=b)
end

function fitnplot(z::Vector; D::Int=1, interpres=100, quadnodes=25, method::Optim.AbstractOptimizer=Newton(), from::NamedTuple=NamedTuple(), xform::NamedTuple=NamedTuple(),
									graphstub="", noplot::Bool=false, symmetric::Bool=false, zplot::StepRangeLen=(symmetric ? 0 : -5):.1:5, Ï‰plot::StepRangeLen=(symmetric ? 0 : -5):.1:5, )

	from = merge((p=fill(1/D,D), Î¼=(symmetric ? Float64[] : fill(0.,D)), Ï„=collect(LinRange(1,D,D)), pFâ‚€=.1, pHâ‚€=[.1,.1], kH=[1., 1.], m=1.), from)
	xform = merge((p=SimplextoRâ¿, Î¼=identity, Ï„=bcast(log), pFâ‚€=logit, pHâ‚€=bcast(logit), kH=bcast(log), m=log), xform)

	M = HnFmodel(z; D, interpres, quadnodes, xform...)
	f = fit(M, from; method)

	one2D = string.(1:D)
	coefnames = vcat("p".* one2D, from.Î¼ isa Number ? "Î¼"	: "Î¼".*one2D, "Ï„".* one2D, "pFâ‚€", "pLâ‚€", "pHâ‚€", "kL", "kH", "m")
	b = vcat(f.b...)
	est = HnFResult(string(:z), coefnames, b, f.V, length(_b), size(z,1), -Optim.minimum(f.res))

	if !noplot
		t = NamedTuple([p=>(iszero(length(b)) && p==:Î¼ ? 0. : b) for ((p,_),b) âˆˆ zip(pairs(from), f.b)])
		kwargsÏ‰ = (p=t.p, Î¼=t.Î¼, Ï„=t.Ï„)
		kwargsz = (pFâ‚€=t.pFâ‚€, pHâ‚€=t.pHâ‚€, kH=t.kH, m=t.m)
		kwargsz0 = (pFâ‚€=0, pHâ‚€=[0.,0.], kH=[0.,0.], m=1)

		plt1 = stephist(z, normalize=:pdf, label="Actual published effects", legend=:topleft)  # outline histogram of data
		s,e = extrema(z); _zplot = s:.1:e
		pplottrue = map(z->t.p' * (@. pdf(Normal(kwargsÏ‰.Î¼,t.Ï„), z)), _zplot)
		pplotinitial = map(z->t.p' * (@. pdf(Normal(kwargsÏ‰.Î¼, âˆš(t.Ï„^2+1)), z)), _zplot)
		pplotfit = map(z->HnFl(z; rtol=1e-2, kwargsÏ‰..., kwargsz...), _zplot)
		if symmetric
			pplottrue .*= 2
			pplotinitial .*= 2
			pplotfit .*= 2
		end
		plot!(_zplot, [pplottrue pplotinitial pplotfit], label=["Model: true effects" "Model: initial estimates" "Model: published estimates"], lw=[1 1 1])
		png("$graphstub fit")

		# distribution of z | Ï‰=2
		Ï‰ = 2.
		plt2 = plot(zplot, mapreduce(z->[fZcondÎ©(z, Ï‰; kwargsz0...) fZcondÎ©.(z, Ï‰; kwargsz...)], vcat, zplot), label=["not distorted" "distorted"], xlabel="Reported z | true z = $Ï‰", lw=[1 1])
		png("$graphstub z cond Ï‰=$Ï‰")
		
		# distribution of Ï‰ | z=2
		_z = 2.
		plt3 = plot(Ï‰plot, mapreduce(Ï‰->[fÎ©condZ(Ï‰,_z; kwargsÏ‰..., kwargsz0...) fÎ©condZ(Ï‰,_z; kwargsÏ‰..., kwargsz...)], vcat, Ï‰plot), label=["not distorted" "distorted"], xlabel="True z | reported z = $_z", lw=[1 1])
		png("$graphstub Ï‰ cond z=$_z")
		
		# frequentist CI's as fn of z
		plt4 = plot(Ï‰plot, mapreduce(Ï‰->[Cquant.([.025 .5 .975], Ï‰; kwargsz0...)..., Cquant.([.025 .5 .975], Ï‰; kwargsz...)...]',vcat,Ï‰plot), linecolor=[:blue :blue :blue :orange :orange :orange], lw=[1 1 1 1 1 1], linestyle=[:solid :dash :solid :solid :dash :solid], legend=false, xlabel="Reported z", ylabel="95% CI & median")
		png("$graphstub CI cond z")
		
		# Bayesian posterior mean of Ï‰ as fn of Z
		pplot = mapreduce(z->[z EÎ©condZ(z; kwargsÏ‰..., kwargsz0...) EÎ©condZ(z; kwargsÏ‰..., kwargsz...)], vcat, zplot)
		plt5 = plot(zplot, pplot, label=["As is" "shrinkage from informative prior" "shrinkage + adjustment for distortion"], xlabel="Reported z", ylabel="Expected true z", lw=[1 1 1])
		png("$graphstub E[Ï‰] cond z")
		
		# E[Ï‰] discount
		plt6 = plot(zplot[zplot.>.2], pplot[zplot.>.2,2:3]./zplot[zplot.>.2], label=["shrinkage from informative prior" "shrinkage + adjustment for distortion"], xlabel="Reported z", ylabel="Discount multiplier", lw=[1 1 1])
		png("$graphstub E[Ï‰] discount")

		plot(plt1, plt2, plt3, plt4, plt5, plt6, size=(2700,1950), dpi=300)
		png("$graphstub all")

		pplot = vcat(map(Ï‰ -> [0. quantFcondÎ©(.5, Ï‰; kwargsz...) - Ï‰], Ï‰plot)...)
		plt7 = plot(Ï‰plot, pplot)
		Cplot = vcat(map(Ï‰ ->[.95 FZcondÎ©(Ï‰+ZÌ„, Ï‰; kwargsz...)-FZcondÎ©(Ï‰-ZÌ„, Ï‰; kwargsz...)], Ï‰plot)...)
		plt8 = plot(Ï‰plot, Cplot)
		plot(plt7, plt8, title="Andrews & Kasy (2019) Figure 1")
		png("$graphstub A&K Fig1")
	end
	est
end


# confirm match between model and simulation
p = [1.]
Î¼ = [0.7]
Ï„ = [2.]
pFâ‚€ = .3
pHâ‚€ = [.2,.3]
kH = [1.,.5]
m = 5.
kwargs = (p=p, Î¼=Î¼, Ï„=Ï„, pFâ‚€=pFâ‚€, pHâ‚€=pHâ‚€, kH=kH, m=m)

Random.seed!(1231)
z = HnFDGP(3_000_000; kwargs..., truncate=true).Z

histogram(z, normalize=:pdf)
zplot = -10:.1:10
pplot = map(z->HnFl(z; kwargs..., truncate=true), zplot)
plot!(zplot, pplot)

M = HnFmodel(z, D=length(Ï„), p=SimplextoRâ¿, Î¼=identity, Ï„=bcast(log), pFâ‚€=logit, pHâ‚€=bcast(logit), kH=bcast(log), m=log)
@time f = fit(M, kwargs)
plot!(zplot, map(z->HnFl(z; f.b...), zplot))


nostar(coef, p) = coef

@time begin
	# Georgescu and Wren 2018 ~1M sample, doi:10.1093/bioinformatics/btx811, github.com/agbarnett/intervals
	df = DataFrame(CSV.File(raw"D:\OneDrive\Documents\Work\Clients & prospects\GiveWell\Noisy data\Georgescu.Wren.csv"))
	@. df.cilevel[ismissing(df.cilevel) || df.cilevel==.0095 || df.cilevel==.05] = .95
	@. df.z = log(df.mean) / (ifelse(ismissing(df.lower) || iszero(df.lower), log(df.upper / df.mean), log(df.upper / df.lower) / 2) / cquantile(Normal(), (1 - df.cilevel)/2.))
	@. @subset!(df, !ismissing(:z) && !ismissing(:lower))
	@. @subset!(df, iszero(:mistake) && abs(:z) < 10.)  # van Zwet & Cator Figure 1 stops at 10
	@. @subset!(df, :source!="Abstract")
	df.z = Float64.(df.z)

	@time fGW1 = fitnplot(df.z; graphstub="Georgescu-Wren")
	@time fGW2 = fitnplot(df.z; D=2, graphstub="Georgescu-Wren 2", method=LBFGS())
	regtable(fGW1, fGW2; estim_decoration=nostar, 
	                     regression_statistics=[:nobs],
											 custom_statistics=(ll=loglikelihood.([fGW1, fGW2]),),
											 print_estimator_section=false,
											 regressors = ["p1", "p2", "Î¼1", "Î¼2", "Ï„1", "Ï„2", "pFâ‚€", "pLâ‚€", "pUâ‚€", "kL", "kU", "m"],
											 estimformat="%0.2g",
											 statisticformat="%0.2g")

	# println("Number of missing studies = ", size(df,1) * pÌ‚F * pÌ‚'*(@. cdf(Normal(Î¼,âˆš(1+Ï„^2)),ZÌ„)-cdf(Normal(Î¼,âˆš(1+Ï„^2)),-ZÌ„)))
	# println("Number of p-hacked studies = ", size(df,1) * (1-pÌ‚D-pÌ‚F) * pÌ‚'*(@. cdf(Normal(Î¼,âˆš(1+Ï„^2)),ZÌ„)-cdf(Normal(Î¼,âˆš(1+Ï„^2)),-ZÌ„)))

	# van Zwet, Schwab, and Senn 2021 data, osf.io/xq4b2
	df = DataFrame(CSV.File(raw"D:\OneDrive\Documents\Work\Clients & prospects\GiveWell\Noisy data\CochraneEffects.csv"))
	@. @subset!(df, abs(:z) < 20 && :"outcome.nr"!=1 && :RCT=="yes" && :"outcome.group"=="efficacy")
	combine(groupby(df, :"study.id.sha1"), :z => sample => :z)  # randomly choose among primary outcomes
	df.z .*= rand([-1. 1.], nrow(df))  # symmetrize data without duplication

	@time fvZSS = fitnplot(df.z; D=4, from=(p=[.32,.31,.3,.07], Î¼=0., Ï„=[.61, 1.42, 2.16, 5.64], pFâ‚€=.01, pHâ‚€=[.01, .01], kH=[0.,0.], m=3.), graphstub="vZSS")

	# Star Wars, doi.org/10.1257/app.20150044, openicpsr.org/openicpsr/project/113633/version/V1/view?path=/openicpsr/113633/fcr:versions/V1/brodeur_le_sangnier_zylberberg_replication/Data/Final/final_stars_supp.dta&type=file
	df = DataFrame(CSV.File(raw"D:\OneDrive\Documents\Work\Library\Meta-science\Brodeur et al. 2016\Data\Final\final_stars_supp.csv"))
	@. @subset!(df, lowercase(:main)=="yes")
	df = DataFrame(Dict(:z => df.coefficient_num ./ df.standard_deviation_num))
	dropmissing!(df)
	@subset!(df, abs.(:z).<10)  # Star Wars graphs stop at 10
	df.z = Float64.(df.z)

	fSW3 = fitnplot(df.z; D=3, xform=(kH=identity,), noplot=true)  # hack: 3-component model converging to higher-likelihood 2-component solution, maybe because of greater flexibility?
	# keep = abs.(fSW3.b.p) .> 1e-3
	# fSW2 = fitnplot(df.z; D=2, xform=(kH=identity,), from=NamedTuple(k=>v isa Vector ? v[keep] : v for (k,v) âˆˆ pairs(fSW3.b)), graphstub="Star Wars 3")
	# fSW4 = fitnplot(df.z; D=4, xform=(kH=identity,), graphstub="Star Wars 4")

	fSW3sym = fitnplot(abs.(df.z); D=3, xform=(pHâ‚€=logit âˆ˜ consvec[2], kH=logit âˆ˜ consvec[2]), symmetric=true, graphstub="fSW3sym")  # symmetrized model

	# Arel-Bundock et al. histogram
	img = load("Arel-Bundock et al. histogram.png")
	hist = [round(Int, 383*(1-(findfirst(<(.5), channelview(img)[1,:,round(Int,c)])-15)/(2049-15))) for c âˆˆ range(13, 3180; length=200)]  # ğŸ˜„
	bar(.025:.05:10, hist)
	z = vcat([fill(z,h) for (z,h) âˆˆ zip(.025:.05:10, hist)]...)
	f1 = fitnplot(z; D=1, xform=(pHâ‚€=logit âˆ˜ consvec[2], kH=logit âˆ˜ consvec[2]), symmetric=true, graphstub="A-B et al. 1")
	f2 = fitnplot(z; D=2, xform=(pHâ‚€=logit âˆ˜ consvec[2], kH=logit âˆ˜ consvec[2]), symmetric=true, graphstub="A-B et al. 2")
	f3 = fitnplot(z; D=3, xform=(pHâ‚€=logit âˆ˜ consvec[2], kH=logit âˆ˜ consvec[2]), symmetric=true, graphstub="A-B et al. 3")

	# Brodeur, Cook, and Heyes 2020, DOI 10.1257/aer.20190687, openicpsr.org/openicpsr/project/120246/version/V1/view?path=/openicpsr/120246/fcr:versions/V1/MM-Data.dta&type=file
	df = DataFrame(CSV.File(raw"D:\OneDrive\Documents\Work\Library\Meta-science\Brodeur, Cook, and Heyes 2020\MM Data.csv"))
	rename!(df, :t => :abst)
	df.t = df.mu ./ df.sd
	@. @subset!(df, abs(:t)<10)
	df.t = Float64.(df.t)
	df.abst = Float64.(df.abst)
	fBCH1 = fitnplot(df.abst; D=1, xform=(pHâ‚€=logit âˆ˜ consvec[2], kH=logit âˆ˜ consvec[2]), symmetric=true, graphstub="BCH 1")
	fBCH2 = fitnplot(df.abst; D=2, xform=(pHâ‚€=logit âˆ˜ consvec[2], kH=logit âˆ˜ consvec[2]), symmetric=true, graphstub="BCH 2")
	fBCH3 = fitnplot(df.abst; D=3, xform=(pHâ‚€=logit âˆ˜ consvec[2], kH=logit âˆ˜ consvec[2]), symmetric=true, graphstub="BCH 3")

	# Vivalt 2020
	df = DataFrame(CSV.File(raw"D:\OneDrive\Documents\Work\Library\Meta-science\Vivalt 2020\Data\data_unstandardized.csv"))
	df.z = df.treatmentcoefficient ./ df.treatmentstandarderror
	@. @subset!(df, abs(:z)<10)
	fV1 = fitnplot(df.z; D=1, graphstub="V 1")
	fV2 = fitnplot(df.z; D=2, graphstub="V 2")
end